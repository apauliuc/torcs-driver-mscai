{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('f-speedway.csv')\n",
    "\n",
    "TARGETS = data.iloc[:,0:3].values\n",
    "INPUTS = data.iloc[:,3:].values\n",
    "\n",
    "TARGET_SIZE = TARGETS.shape[1]\n",
    "INPUT_SIZE = INPUTS.shape[1]\n",
    "HIDDEN_SIZE = INPUT_SIZE\n",
    "\n",
    "NUM_LAYERS = 1\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 500\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "def normalize(x, mmin, mmax):\n",
    "    return clip(x - min(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMDriver(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, batch_size):\n",
    "        super(LSTMDriver, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        super(LSTMDriver, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers)\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        self.linear = nn.Linear(self.input_size, self.output_size)\n",
    "\n",
    "#         self.out = nn.Sigmoid()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size), requires_grad=False),\n",
    "                    autograd.Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)), requires_grad=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, hidden_out = self.lstm(x.view(1, -1), self.hidden)\n",
    "        linear_out = self.linear(lstm_out)\n",
    "        self.hidden = (autograd.Variable(hidden_out[0].data), autograd.Variable(hidden_out[1].data))\n",
    "#         out = self.out(linear_out)\n",
    "        \n",
    "        return linear_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] | Loss: 5.938199\n",
      "Epoch [2/500] | Loss: 6.886178\n",
      "Epoch [3/500] | Loss: 7.286763\n",
      "Epoch [4/500] | Loss: 13.402437\n",
      "Epoch [5/500] | Loss: 13.394083\n",
      "Epoch [6/500] | Loss: 13.391360\n",
      "Epoch [7/500] | Loss: 13.391040\n",
      "Epoch [8/500] | Loss: 13.389775\n",
      "Epoch [9/500] | Loss: 13.389166\n",
      "Epoch [10/500] | Loss: 13.388746\n",
      "Epoch [11/500] | Loss: 13.387579\n",
      "Epoch [12/500] | Loss: 13.386180\n",
      "Epoch [13/500] | Loss: 13.332379\n",
      "Epoch [14/500] | Loss: 13.308661\n",
      "Epoch [15/500] | Loss: 13.300665\n",
      "Epoch [16/500] | Loss: 13.310490\n",
      "Epoch [17/500] | Loss: 13.310289\n",
      "Epoch [18/500] | Loss: 13.300524\n",
      "Epoch [19/500] | Loss: 13.380461\n",
      "Epoch [20/500] | Loss: 13.382790\n",
      "Epoch [21/500] | Loss: 13.382375\n",
      "Epoch [22/500] | Loss: 13.382201\n",
      "Epoch [23/500] | Loss: 13.381989\n",
      "Epoch [24/500] | Loss: 13.381812\n",
      "Epoch [25/500] | Loss: 13.381651\n",
      "Epoch [26/500] | Loss: 13.381504\n",
      "Epoch [27/500] | Loss: 13.381371\n",
      "Epoch [28/500] | Loss: 13.381249\n",
      "Epoch [29/500] | Loss: 13.381135\n",
      "Epoch [30/500] | Loss: 13.381029\n",
      "Epoch [31/500] | Loss: 13.380929\n",
      "Epoch [32/500] | Loss: 13.380834\n",
      "Epoch [33/500] | Loss: 13.380744\n",
      "Epoch [34/500] | Loss: 13.380658\n",
      "Epoch [35/500] | Loss: 13.380576\n",
      "Epoch [36/500] | Loss: 13.380500\n",
      "Epoch [37/500] | Loss: 13.380431\n",
      "Epoch [38/500] | Loss: 13.380369\n",
      "Epoch [39/500] | Loss: 13.380317\n",
      "Epoch [40/500] | Loss: 13.380275\n",
      "Epoch [41/500] | Loss: 13.380244\n",
      "Epoch [42/500] | Loss: 13.380222\n",
      "Epoch [43/500] | Loss: 13.380210\n",
      "Epoch [44/500] | Loss: 13.380205\n",
      "Epoch [45/500] | Loss: 13.380204\n",
      "Epoch [46/500] | Loss: 13.380206\n",
      "Epoch [47/500] | Loss: 13.380209\n",
      "Epoch [48/500] | Loss: 13.380212\n",
      "Epoch [49/500] | Loss: 13.380213\n",
      "Epoch [50/500] | Loss: 13.380213\n",
      "Epoch [51/500] | Loss: 13.380212\n",
      "Epoch [52/500] | Loss: 13.380209\n",
      "Epoch [53/500] | Loss: 13.380206\n",
      "Epoch [54/500] | Loss: 13.380202\n",
      "Epoch [55/500] | Loss: 13.380199\n",
      "Epoch [56/500] | Loss: 13.380195\n",
      "Epoch [57/500] | Loss: 13.380192\n",
      "Epoch [58/500] | Loss: 13.380189\n",
      "Epoch [59/500] | Loss: 13.380186\n",
      "Epoch [60/500] | Loss: 13.380184\n",
      "Epoch [61/500] | Loss: 13.380182\n",
      "Epoch [62/500] | Loss: 13.380180\n",
      "Epoch [63/500] | Loss: 13.380178\n",
      "Epoch [64/500] | Loss: 13.380176\n",
      "Epoch [65/500] | Loss: 13.380173\n",
      "Epoch [66/500] | Loss: 13.380170\n",
      "Epoch [67/500] | Loss: 13.380168\n",
      "Epoch [68/500] | Loss: 13.380165\n",
      "Epoch [69/500] | Loss: 13.380162\n",
      "Epoch [70/500] | Loss: 13.380158\n",
      "Epoch [71/500] | Loss: 13.380155\n",
      "Epoch [72/500] | Loss: 13.380152\n",
      "Epoch [73/500] | Loss: 13.380148\n",
      "Epoch [74/500] | Loss: 13.380145\n",
      "Epoch [75/500] | Loss: 13.380141\n",
      "Epoch [76/500] | Loss: 13.380137\n",
      "Epoch [77/500] | Loss: 13.380133\n",
      "Epoch [78/500] | Loss: 13.380128\n",
      "Epoch [79/500] | Loss: 13.380122\n",
      "Epoch [80/500] | Loss: 13.380115\n",
      "Epoch [81/500] | Loss: 13.380105\n",
      "Epoch [82/500] | Loss: 13.380092\n",
      "Epoch [83/500] | Loss: 13.380075\n",
      "Epoch [84/500] | Loss: 13.380058\n",
      "Epoch [85/500] | Loss: 13.380048\n",
      "Epoch [86/500] | Loss: 13.380058\n",
      "Epoch [87/500] | Loss: 13.380110\n",
      "Epoch [88/500] | Loss: 13.380147\n",
      "Epoch [89/500] | Loss: 13.380138\n",
      "Epoch [90/500] | Loss: 13.380137\n",
      "Epoch [91/500] | Loss: 13.380135\n",
      "Epoch [92/500] | Loss: 13.380133\n",
      "Epoch [93/500] | Loss: 13.380131\n",
      "Epoch [94/500] | Loss: 13.380129\n",
      "Epoch [95/500] | Loss: 13.380128\n",
      "Epoch [96/500] | Loss: 13.380127\n",
      "Epoch [97/500] | Loss: 13.380125\n",
      "Epoch [98/500] | Loss: 13.380124\n",
      "Epoch [99/500] | Loss: 13.380123\n",
      "Epoch [100/500] | Loss: 13.380122\n",
      "Epoch [101/500] | Loss: 13.380121\n",
      "Epoch [102/500] | Loss: 13.380121\n",
      "Epoch [103/500] | Loss: 13.380120\n",
      "Epoch [104/500] | Loss: 13.380118\n",
      "Epoch [105/500] | Loss: 13.380118\n",
      "Epoch [106/500] | Loss: 13.380117\n",
      "Epoch [107/500] | Loss: 13.380117\n",
      "Epoch [108/500] | Loss: 13.380116\n",
      "Epoch [109/500] | Loss: 13.380115\n",
      "Epoch [110/500] | Loss: 13.380115\n",
      "Epoch [111/500] | Loss: 13.380114\n",
      "Epoch [112/500] | Loss: 13.380114\n",
      "Epoch [113/500] | Loss: 13.380113\n",
      "Epoch [114/500] | Loss: 13.380113\n",
      "Epoch [115/500] | Loss: 13.380112\n",
      "Epoch [116/500] | Loss: 13.380112\n",
      "Epoch [117/500] | Loss: 13.380111\n",
      "Epoch [118/500] | Loss: 13.380110\n",
      "Epoch [119/500] | Loss: 13.380109\n",
      "Epoch [120/500] | Loss: 13.380108\n",
      "Epoch [121/500] | Loss: 13.380107\n",
      "Epoch [122/500] | Loss: 13.380103\n",
      "Epoch [123/500] | Loss: 13.380066\n",
      "Epoch [124/500] | Loss: 13.380062\n",
      "Epoch [125/500] | Loss: 13.380058\n",
      "Epoch [126/500] | Loss: 13.379961\n",
      "Epoch [127/500] | Loss: 13.379716\n",
      "Epoch [128/500] | Loss: 13.377873\n",
      "Epoch [129/500] | Loss: 13.415664\n",
      "Epoch [130/500] | Loss: 13.459557\n",
      "Epoch [131/500] | Loss: 13.456267\n",
      "Epoch [132/500] | Loss: 13.454745\n",
      "Epoch [133/500] | Loss: 13.455364\n",
      "Epoch [134/500] | Loss: 12.827048\n",
      "Epoch [135/500] | Loss: 12.266131\n",
      "Epoch [136/500] | Loss: 13.444367\n",
      "Epoch [137/500] | Loss: 5.142350\n",
      "Epoch [138/500] | Loss: 8.298923\n",
      "Epoch [139/500] | Loss: 13.344303\n",
      "Epoch [140/500] | Loss: 13.303685\n",
      "Epoch [141/500] | Loss: 13.303376\n",
      "Epoch [142/500] | Loss: 13.303124\n",
      "Epoch [143/500] | Loss: 13.302897\n",
      "Epoch [144/500] | Loss: 13.302688\n",
      "Epoch [145/500] | Loss: 13.302496\n",
      "Epoch [146/500] | Loss: 13.302322\n",
      "Epoch [147/500] | Loss: 13.302161\n",
      "Epoch [148/500] | Loss: 13.302009\n",
      "Epoch [149/500] | Loss: 13.301863\n",
      "Epoch [150/500] | Loss: 13.301722\n",
      "Epoch [151/500] | Loss: 13.301584\n",
      "Epoch [152/500] | Loss: 13.301453\n",
      "Epoch [153/500] | Loss: 13.301333\n",
      "Epoch [154/500] | Loss: 13.301230\n",
      "Epoch [155/500] | Loss: 13.301147\n",
      "Epoch [156/500] | Loss: 13.301088\n",
      "Epoch [157/500] | Loss: 13.301052\n",
      "Epoch [158/500] | Loss: 13.301036\n",
      "Epoch [159/500] | Loss: 13.301036\n",
      "Epoch [160/500] | Loss: 13.301045\n",
      "Epoch [161/500] | Loss: 13.301058\n",
      "Epoch [162/500] | Loss: 13.301069\n",
      "Epoch [163/500] | Loss: 13.301077\n",
      "Epoch [164/500] | Loss: 13.301081\n",
      "Epoch [165/500] | Loss: 13.301080\n",
      "Epoch [166/500] | Loss: 13.301075\n",
      "Epoch [167/500] | Loss: 13.301068\n",
      "Epoch [168/500] | Loss: 13.301059\n",
      "Epoch [169/500] | Loss: 13.301049\n",
      "Epoch [170/500] | Loss: 13.301040\n",
      "Epoch [171/500] | Loss: 13.301032\n",
      "Epoch [172/500] | Loss: 13.301025\n",
      "Epoch [173/500] | Loss: 13.301019\n",
      "Epoch [174/500] | Loss: 13.301014\n",
      "Epoch [175/500] | Loss: 13.301009\n",
      "Epoch [176/500] | Loss: 13.301006\n",
      "Epoch [177/500] | Loss: 13.301002\n",
      "Epoch [178/500] | Loss: 13.300999\n",
      "Epoch [179/500] | Loss: 13.300997\n",
      "Epoch [180/500] | Loss: 13.300993\n",
      "Epoch [181/500] | Loss: 13.300990\n",
      "Epoch [182/500] | Loss: 13.300987\n",
      "Epoch [183/500] | Loss: 13.300983\n",
      "Epoch [184/500] | Loss: 13.300980\n",
      "Epoch [185/500] | Loss: 13.300978\n",
      "Epoch [186/500] | Loss: 13.300974\n",
      "Epoch [187/500] | Loss: 13.300970\n",
      "Epoch [188/500] | Loss: 13.300967\n",
      "Epoch [189/500] | Loss: 13.300964\n",
      "Epoch [190/500] | Loss: 13.300961\n",
      "Epoch [191/500] | Loss: 13.300958\n",
      "Epoch [192/500] | Loss: 13.300955\n",
      "Epoch [193/500] | Loss: 13.300952\n",
      "Epoch [194/500] | Loss: 13.300948\n",
      "Epoch [195/500] | Loss: 13.300946\n",
      "Epoch [196/500] | Loss: 13.300943\n",
      "Epoch [197/500] | Loss: 13.300939\n",
      "Epoch [198/500] | Loss: 13.300936\n",
      "Epoch [199/500] | Loss: 13.300932\n",
      "Epoch [200/500] | Loss: 13.300929\n",
      "Epoch [201/500] | Loss: 13.300925\n",
      "Epoch [202/500] | Loss: 13.300921\n",
      "Epoch [203/500] | Loss: 13.300916\n",
      "Epoch [204/500] | Loss: 13.300911\n",
      "Epoch [205/500] | Loss: 13.300905\n",
      "Epoch [206/500] | Loss: 13.300899\n",
      "Epoch [207/500] | Loss: 13.300892\n",
      "Epoch [208/500] | Loss: 13.300884\n",
      "Epoch [209/500] | Loss: 13.300875\n",
      "Epoch [210/500] | Loss: 13.300866\n",
      "Epoch [211/500] | Loss: 13.300858\n",
      "Epoch [212/500] | Loss: 13.300849\n",
      "Epoch [213/500] | Loss: 13.300840\n",
      "Epoch [214/500] | Loss: 13.300831\n",
      "Epoch [215/500] | Loss: 13.300823\n",
      "Epoch [216/500] | Loss: 13.300815\n",
      "Epoch [217/500] | Loss: 13.300807\n",
      "Epoch [218/500] | Loss: 13.300800\n",
      "Epoch [219/500] | Loss: 13.300792\n",
      "Epoch [220/500] | Loss: 13.300784\n",
      "Epoch [221/500] | Loss: 13.300775\n",
      "Epoch [222/500] | Loss: 13.300765\n",
      "Epoch [223/500] | Loss: 13.300755\n",
      "Epoch [224/500] | Loss: 13.300742\n",
      "Epoch [225/500] | Loss: 13.300729\n",
      "Epoch [226/500] | Loss: 13.300713\n",
      "Epoch [227/500] | Loss: 13.300693\n",
      "Epoch [228/500] | Loss: 13.300896\n",
      "Epoch [229/500] | Loss: 13.300436\n",
      "Epoch [230/500] | Loss: 13.300650\n",
      "Epoch [231/500] | Loss: 12.428786\n",
      "Epoch [232/500] | Loss: 13.300799\n",
      "Epoch [233/500] | Loss: 11.934071\n",
      "Epoch [234/500] | Loss: 12.039842\n",
      "Epoch [235/500] | Loss: 13.257501\n",
      "Epoch [236/500] | Loss: 4.389999\n",
      "Epoch [237/500] | Loss: 13.385224\n",
      "Epoch [238/500] | Loss: 13.386711\n",
      "Epoch [239/500] | Loss: 13.382883\n",
      "Epoch [240/500] | Loss: 13.385108\n",
      "Epoch [241/500] | Loss: 13.382016\n",
      "Epoch [242/500] | Loss: 13.383709\n",
      "Epoch [243/500] | Loss: 13.380839\n",
      "Epoch [244/500] | Loss: 13.382556\n",
      "Epoch [245/500] | Loss: 13.380025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [246/500] | Loss: 13.381778\n",
      "Epoch [247/500] | Loss: 13.379638\n",
      "Epoch [248/500] | Loss: 13.380138\n",
      "Epoch [249/500] | Loss: 13.377541\n",
      "Epoch [250/500] | Loss: 13.372547\n",
      "Epoch [251/500] | Loss: 13.372545\n",
      "Epoch [252/500] | Loss: 13.372542\n",
      "Epoch [253/500] | Loss: 13.372537\n",
      "Epoch [254/500] | Loss: 13.372532\n",
      "Epoch [255/500] | Loss: 13.372520\n",
      "Epoch [256/500] | Loss: 13.372498\n",
      "Epoch [257/500] | Loss: 13.372403\n",
      "Epoch [258/500] | Loss: 13.595721\n",
      "Epoch [259/500] | Loss: 13.372562\n",
      "Epoch [260/500] | Loss: 13.372562\n",
      "Epoch [261/500] | Loss: 13.372562\n",
      "Epoch [262/500] | Loss: 13.372562\n",
      "Epoch [263/500] | Loss: 13.372562\n",
      "Epoch [264/500] | Loss: 13.372562\n",
      "Epoch [265/500] | Loss: 13.372561\n",
      "Epoch [266/500] | Loss: 13.372562\n",
      "Epoch [267/500] | Loss: 13.372561\n",
      "Epoch [268/500] | Loss: 13.372561\n",
      "Epoch [269/500] | Loss: 13.372561\n",
      "Epoch [270/500] | Loss: 13.372562\n",
      "Epoch [271/500] | Loss: 13.372561\n",
      "Epoch [272/500] | Loss: 13.372561\n",
      "Epoch [273/500] | Loss: 13.372561\n",
      "Epoch [274/500] | Loss: 13.372561\n",
      "Epoch [275/500] | Loss: 13.372561\n",
      "Epoch [276/500] | Loss: 13.372561\n",
      "Epoch [277/500] | Loss: 13.372561\n",
      "Epoch [278/500] | Loss: 13.372561\n",
      "Epoch [279/500] | Loss: 13.372562\n",
      "Epoch [280/500] | Loss: 13.372561\n",
      "Epoch [281/500] | Loss: 13.372561\n",
      "Epoch [282/500] | Loss: 13.372561\n",
      "Epoch [283/500] | Loss: 13.372561\n",
      "Epoch [284/500] | Loss: 13.372561\n",
      "Epoch [285/500] | Loss: 13.372561\n",
      "Epoch [286/500] | Loss: 13.372561\n",
      "Epoch [287/500] | Loss: 13.372561\n",
      "Epoch [288/500] | Loss: 13.372561\n",
      "Epoch [289/500] | Loss: 13.372562\n",
      "Epoch [290/500] | Loss: 13.372561\n",
      "Epoch [291/500] | Loss: 13.372561\n",
      "Epoch [292/500] | Loss: 13.372561\n",
      "Epoch [293/500] | Loss: 13.372561\n",
      "Epoch [294/500] | Loss: 13.372561\n",
      "Epoch [295/500] | Loss: 13.372561\n",
      "Epoch [296/500] | Loss: 13.372561\n",
      "Epoch [297/500] | Loss: 13.372561\n",
      "Epoch [298/500] | Loss: 13.372561\n",
      "Epoch [299/500] | Loss: 13.372560\n",
      "Epoch [300/500] | Loss: 13.372561\n",
      "Epoch [301/500] | Loss: 13.372561\n",
      "Epoch [302/500] | Loss: 13.372561\n",
      "Epoch [303/500] | Loss: 13.372560\n",
      "Epoch [304/500] | Loss: 13.372561\n",
      "Epoch [305/500] | Loss: 13.372561\n",
      "Epoch [306/500] | Loss: 13.372561\n",
      "Epoch [307/500] | Loss: 13.372561\n",
      "Epoch [308/500] | Loss: 13.372560\n",
      "Epoch [309/500] | Loss: 13.372561\n",
      "Epoch [310/500] | Loss: 13.372560\n",
      "Epoch [311/500] | Loss: 13.372561\n",
      "Epoch [312/500] | Loss: 13.372560\n",
      "Epoch [313/500] | Loss: 13.372560\n",
      "Epoch [314/500] | Loss: 13.372561\n",
      "Epoch [315/500] | Loss: 13.372561\n",
      "Epoch [316/500] | Loss: 13.372560\n",
      "Epoch [317/500] | Loss: 13.372561\n",
      "Epoch [318/500] | Loss: 13.372561\n",
      "Epoch [319/500] | Loss: 13.372561\n",
      "Epoch [320/500] | Loss: 13.372560\n",
      "Epoch [321/500] | Loss: 13.372561\n",
      "Epoch [322/500] | Loss: 13.372561\n",
      "Epoch [323/500] | Loss: 13.372561\n",
      "Epoch [324/500] | Loss: 13.372561\n",
      "Epoch [325/500] | Loss: 13.372560\n",
      "Epoch [326/500] | Loss: 13.372560\n",
      "Epoch [327/500] | Loss: 13.372561\n",
      "Epoch [328/500] | Loss: 13.372561\n",
      "Epoch [329/500] | Loss: 13.372560\n",
      "Epoch [330/500] | Loss: 13.372560\n",
      "Epoch [331/500] | Loss: 13.372561\n",
      "Epoch [332/500] | Loss: 13.372560\n",
      "Epoch [333/500] | Loss: 13.372560\n",
      "Epoch [334/500] | Loss: 13.372561\n",
      "Epoch [335/500] | Loss: 13.372560\n",
      "Epoch [336/500] | Loss: 13.372560\n",
      "Epoch [337/500] | Loss: 13.372560\n",
      "Epoch [338/500] | Loss: 13.372560\n",
      "Epoch [339/500] | Loss: 13.372560\n",
      "Epoch [340/500] | Loss: 13.372560\n",
      "Epoch [341/500] | Loss: 13.372561\n",
      "Epoch [342/500] | Loss: 13.372560\n",
      "Epoch [343/500] | Loss: 13.372560\n",
      "Epoch [344/500] | Loss: 13.372560\n",
      "Epoch [345/500] | Loss: 13.372560\n",
      "Epoch [346/500] | Loss: 13.372560\n",
      "Epoch [347/500] | Loss: 13.372561\n",
      "Epoch [348/500] | Loss: 13.372560\n",
      "Epoch [349/500] | Loss: 13.372560\n",
      "Epoch [350/500] | Loss: 13.372561\n",
      "Epoch [351/500] | Loss: 13.372560\n",
      "Epoch [352/500] | Loss: 13.372560\n",
      "Epoch [353/500] | Loss: 13.372560\n",
      "Epoch [354/500] | Loss: 13.372560\n",
      "Epoch [355/500] | Loss: 13.372560\n",
      "Epoch [356/500] | Loss: 13.372560\n",
      "Epoch [357/500] | Loss: 13.372560\n",
      "Epoch [358/500] | Loss: 13.372559\n",
      "Epoch [359/500] | Loss: 13.372560\n",
      "Epoch [360/500] | Loss: 13.372560\n",
      "Epoch [361/500] | Loss: 13.372560\n",
      "Epoch [362/500] | Loss: 13.372560\n",
      "Epoch [363/500] | Loss: 13.372560\n",
      "Epoch [364/500] | Loss: 13.372560\n",
      "Epoch [365/500] | Loss: 13.372560\n",
      "Epoch [366/500] | Loss: 13.372560\n",
      "Epoch [367/500] | Loss: 13.372560\n",
      "Epoch [368/500] | Loss: 13.372560\n",
      "Epoch [369/500] | Loss: 13.372560\n",
      "Epoch [370/500] | Loss: 13.372560\n",
      "Epoch [371/500] | Loss: 13.372560\n",
      "Epoch [372/500] | Loss: 13.372560\n",
      "Epoch [373/500] | Loss: 13.372560\n",
      "Epoch [374/500] | Loss: 13.372560\n",
      "Epoch [375/500] | Loss: 13.372561\n",
      "Epoch [376/500] | Loss: 13.372560\n",
      "Epoch [377/500] | Loss: 13.372560\n",
      "Epoch [378/500] | Loss: 13.372560\n",
      "Epoch [379/500] | Loss: 13.372561\n",
      "Epoch [380/500] | Loss: 13.372560\n",
      "Epoch [381/500] | Loss: 13.372559\n",
      "Epoch [382/500] | Loss: 13.372560\n",
      "Epoch [383/500] | Loss: 13.372560\n",
      "Epoch [384/500] | Loss: 13.372560\n",
      "Epoch [385/500] | Loss: 13.372560\n",
      "Epoch [386/500] | Loss: 13.372560\n",
      "Epoch [387/500] | Loss: 13.372560\n",
      "Epoch [388/500] | Loss: 13.372560\n",
      "Epoch [389/500] | Loss: 13.372560\n",
      "Epoch [390/500] | Loss: 13.372560\n",
      "Epoch [391/500] | Loss: 13.372560\n",
      "Epoch [392/500] | Loss: 13.372560\n",
      "Epoch [393/500] | Loss: 13.372559\n",
      "Epoch [394/500] | Loss: 13.372560\n",
      "Epoch [395/500] | Loss: 13.372560\n",
      "Epoch [396/500] | Loss: 13.372560\n",
      "Epoch [397/500] | Loss: 13.372560\n",
      "Epoch [398/500] | Loss: 13.372560\n",
      "Epoch [399/500] | Loss: 13.372560\n",
      "Epoch [400/500] | Loss: 13.372559\n",
      "Epoch [401/500] | Loss: 13.372560\n",
      "Epoch [402/500] | Loss: 13.372559\n",
      "Epoch [403/500] | Loss: 13.372560\n",
      "Epoch [404/500] | Loss: 13.372559\n",
      "Epoch [405/500] | Loss: 13.372560\n",
      "Epoch [406/500] | Loss: 13.372559\n",
      "Epoch [407/500] | Loss: 13.372560\n",
      "Epoch [408/500] | Loss: 13.372559\n",
      "Epoch [409/500] | Loss: 13.372560\n",
      "Epoch [410/500] | Loss: 13.372559\n",
      "Epoch [411/500] | Loss: 13.372560\n",
      "Epoch [412/500] | Loss: 13.372560\n",
      "Epoch [413/500] | Loss: 13.372560\n",
      "Epoch [414/500] | Loss: 13.372559\n",
      "Epoch [415/500] | Loss: 13.372560\n",
      "Epoch [416/500] | Loss: 13.372559\n",
      "Epoch [417/500] | Loss: 13.372560\n",
      "Epoch [418/500] | Loss: 13.372560\n",
      "Epoch [419/500] | Loss: 13.372560\n",
      "Epoch [420/500] | Loss: 13.372559\n",
      "Epoch [421/500] | Loss: 13.372560\n",
      "Epoch [422/500] | Loss: 13.372560\n",
      "Epoch [423/500] | Loss: 13.372559\n",
      "Epoch [424/500] | Loss: 13.372559\n",
      "Epoch [425/500] | Loss: 13.372560\n",
      "Epoch [426/500] | Loss: 13.372560\n",
      "Epoch [427/500] | Loss: 13.372559\n",
      "Epoch [428/500] | Loss: 13.372560\n",
      "Epoch [429/500] | Loss: 13.372561\n",
      "Epoch [430/500] | Loss: 13.372559\n",
      "Epoch [431/500] | Loss: 13.372560\n",
      "Epoch [432/500] | Loss: 13.372559\n",
      "Epoch [433/500] | Loss: 13.372560\n",
      "Epoch [434/500] | Loss: 13.372560\n",
      "Epoch [435/500] | Loss: 13.372560\n",
      "Epoch [436/500] | Loss: 13.372559\n",
      "Epoch [437/500] | Loss: 13.372559\n",
      "Epoch [438/500] | Loss: 13.372559\n",
      "Epoch [439/500] | Loss: 13.372560\n",
      "Epoch [440/500] | Loss: 13.372559\n",
      "Epoch [441/500] | Loss: 13.372559\n",
      "Epoch [442/500] | Loss: 13.372559\n",
      "Epoch [443/500] | Loss: 13.372560\n",
      "Epoch [444/500] | Loss: 13.372559\n",
      "Epoch [445/500] | Loss: 13.372559\n",
      "Epoch [446/500] | Loss: 13.372559\n",
      "Epoch [447/500] | Loss: 13.372559\n",
      "Epoch [448/500] | Loss: 13.372559\n",
      "Epoch [449/500] | Loss: 13.372559\n",
      "Epoch [450/500] | Loss: 13.372559\n",
      "Epoch [451/500] | Loss: 13.372559\n",
      "Epoch [452/500] | Loss: 13.372559\n",
      "Epoch [453/500] | Loss: 13.372559\n",
      "Epoch [454/500] | Loss: 13.372559\n",
      "Epoch [455/500] | Loss: 13.372559\n",
      "Epoch [456/500] | Loss: 13.372560\n",
      "Epoch [457/500] | Loss: 13.372559\n",
      "Epoch [458/500] | Loss: 13.372559\n",
      "Epoch [459/500] | Loss: 13.372559\n",
      "Epoch [460/500] | Loss: 13.372559\n",
      "Epoch [461/500] | Loss: 13.372559\n",
      "Epoch [462/500] | Loss: 13.372559\n",
      "Epoch [463/500] | Loss: 13.372559\n",
      "Epoch [464/500] | Loss: 13.372560\n",
      "Epoch [465/500] | Loss: 13.372560\n",
      "Epoch [466/500] | Loss: 13.372560\n",
      "Epoch [467/500] | Loss: 13.372560\n",
      "Epoch [468/500] | Loss: 13.372559\n",
      "Epoch [469/500] | Loss: 13.372560\n",
      "Epoch [470/500] | Loss: 13.372560\n",
      "Epoch [471/500] | Loss: 13.372559\n",
      "Epoch [472/500] | Loss: 13.372559\n",
      "Epoch [473/500] | Loss: 13.372558\n",
      "Epoch [474/500] | Loss: 13.372560\n",
      "Epoch [475/500] | Loss: 13.372559\n",
      "Epoch [476/500] | Loss: 13.372559\n",
      "Epoch [477/500] | Loss: 13.372559\n",
      "Epoch [478/500] | Loss: 13.372559\n",
      "Epoch [479/500] | Loss: 13.372559\n",
      "Epoch [480/500] | Loss: 13.372559\n",
      "Epoch [481/500] | Loss: 13.372560\n",
      "Epoch [482/500] | Loss: 13.372559\n",
      "Epoch [483/500] | Loss: 13.372559\n",
      "Epoch [484/500] | Loss: 13.372560\n",
      "Epoch [485/500] | Loss: 13.372559\n",
      "Epoch [486/500] | Loss: 13.372559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [487/500] | Loss: 13.372558\n",
      "Epoch [488/500] | Loss: 13.372559\n",
      "Epoch [489/500] | Loss: 13.372559\n",
      "Epoch [490/500] | Loss: 13.372559\n",
      "Epoch [491/500] | Loss: 13.372560\n",
      "Epoch [492/500] | Loss: 13.372559\n",
      "Epoch [493/500] | Loss: 13.372559\n",
      "Epoch [494/500] | Loss: 13.372559\n",
      "Epoch [495/500] | Loss: 13.372559\n",
      "Epoch [496/500] | Loss: 13.372559\n",
      "Epoch [497/500] | Loss: 13.372559\n",
      "Epoch [498/500] | Loss: 13.372559\n",
      "Epoch [499/500] | Loss: 13.372560\n",
      "Epoch [500/500] | Loss: 13.372559\n"
     ]
    }
   ],
   "source": [
    "model = LSTMDriver(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, TARGET_SIZE, BATCH_SIZE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in np.arange(NUM_EPOCHS):\n",
    "    loss_val = 0\n",
    "    for row in data.values:\n",
    "        model.zero_grad()\n",
    "        targets = autograd.Variable(torch.Tensor(row[0:3]))\n",
    "        inputs = autograd.Variable(torch.Tensor(normalize(row[3:], 0, 1))\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_val += loss.data[0]\n",
    "    print('Epoch [%d/%d] | Loss: %f' %(epoch+1, NUM_EPOCHS, loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type LSTMDriver. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'rnn_params.pt')\n",
    "torch.save(model, 'whole_net.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
