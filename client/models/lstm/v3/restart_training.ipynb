{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import steering\n",
    "import speeding\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = '../../../..'\n",
    "LEARNED_DRIVER = 'snakeoil_miner/data'\n",
    "DIFFICULTY = ['easy', 'medium', 'hard']\n",
    "\n",
    "# COMMAND = 'steering'\n",
    "COMMAND = 'speeding'\n",
    "logging.basicConfig(filename='logs/{}/training-{}.log'.format(COMMAND, time.time()),level=logging.DEBUG)\n",
    "\n",
    "NUM_EPOCHS = 400\n",
    "\n",
    "TRAINING_FILES = []\n",
    "for d in DIFFICULTY:\n",
    "    TRAINING_FILES.extend(glob.glob('/'.join([PROJECT_ROOT, LEARNED_DRIVER, d, '*.csv'])))\n",
    "\n",
    "TRAINING_DATA = {}\n",
    "for FILE in TRAINING_FILES:\n",
    "    DF = pd.read_csv(FILE, index_col=False)\n",
    "    TRAINING_DATA[FILE] = DF.values\n",
    "    \n",
    "CUDA = torch.cuda.is_available()\n",
    "if CUDA:\n",
    "    DTYPE = torch.cuda.FloatTensor\n",
    "else:\n",
    "    DTYPE = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filepath='latest_checkpoint.tar'):\n",
    "    torch.save(state, 'split_checkpoints/' + COMMAND + '/' + filepath)\n",
    "    if is_best:\n",
    "        torch.save(state, 'split_checkpoints/' + COMMAND + '/' + 'best_checkpoint.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(training_data, model, criterion, optimzier):\n",
    "    loss = 0\n",
    "    model.train(mode=True)\n",
    "    for key in training_data:\n",
    "        logging.info('--- Parsing track {}-{}'.format(key.split('/')[-2], key.split('/')[-1]))\n",
    "        print('--- Parsing track {}-{}'.format(key.split('/')[-2], key.split('/')[-1]))\n",
    "        \n",
    "        model.hidden_state = model.init_hidden()\n",
    "        optimizer.zero_grad()\n",
    "        track_sequence = training_data[key]\n",
    "\n",
    "        if COMMAND == 'steering':\n",
    "            targets = track_sequence[:, 2:3]\n",
    "        elif COMMAND == 'speeding':\n",
    "            targets = track_sequence[:, 0:2]\n",
    "        \n",
    "        inputs = track_sequence[:, 3:]\n",
    "\n",
    "        targets_variable = autograd.Variable(torch.Tensor(targets)).type(DTYPE)\n",
    "        inputs_variable = autograd.Variable(torch.Tensor(inputs),  requires_grad=True).type(DTYPE)\n",
    "\n",
    "        outputs_variable = model(inputs_variable)\n",
    "\n",
    "        track_loss = criterion(outputs_variable, targets_variable)\n",
    "\n",
    "        track_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += track_loss.data[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(validation_data, model, criterion):\n",
    "    loss = 0\n",
    "    model.train(mode=False)\n",
    "    for key in validation_data:\n",
    "        logging.info('--- Parsing track {}-{}'.format(key.split('/')[-2], key.split('/')[-1]))\n",
    "        print('--- Parsing track {}-{}'.format(key.split('/')[-2], key.split('/')[-1]))\n",
    "        \n",
    "        model.hidden_state = model.init_hidden()\n",
    "        track_sequence = validation_data[key]\n",
    "\n",
    "        if COMMAND == 'steering':\n",
    "            targets = track_sequence[:, 2:3]\n",
    "        elif COMMAND == 'speeding':\n",
    "            targets = track_sequence[:, 0:2]\n",
    "            \n",
    "        inputs = track_sequence[:, 3:]\n",
    "\n",
    "        targets_variable = autograd.Variable(torch.Tensor(targets), volatile=True).type(DTYPE)\n",
    "        inputs_variable = autograd.Variable(torch.Tensor(inputs), volatile=True).type(DTYPE)\n",
    "\n",
    "        outputs_variable = model(inputs_variable)\n",
    "\n",
    "        track_loss = criterion(outputs_variable, targets_variable)\n",
    "\n",
    "        loss += track_loss.data[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('split_checkpoints/{}/latest_checkpoint.tar'.format(COMMAND))\n",
    "if COMMAND == 'steering':\n",
    "    model = steering.Steering(steering.HYPERPARAMS.INPUT_SIZE,\n",
    "                               steering.HYPERPARAMS.LSTM_HIDDEN_SIZE,\n",
    "                               steering.HYPERPARAMS.HIDDEN_LAYER_SIZE,\n",
    "                               steering.HYPERPARAMS.DROPOUT_PROB,\n",
    "                               steering.HYPERPARAMS.NUM_LAYERS,\n",
    "                               steering.HYPERPARAMS.TARGET_SIZE,\n",
    "                               steering.HYPERPARAMS.BATCH_SIZE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=steering.HYPERPARAMS.LEARNING_RATE)\n",
    "elif COMMAND == 'speeding':\n",
    "    model = speeding.Speeding(speeding.HYPERPARAMS.INPUT_SIZE,\n",
    "                               speeding.HYPERPARAMS.LSTM_HIDDEN_SIZE,\n",
    "                               speeding.HYPERPARAMS.HIDDEN_LAYER_SIZE,\n",
    "                               speeding.HYPERPARAMS.DROPOUT_PROB,\n",
    "                               speeding.HYPERPARAMS.NUM_LAYERS,\n",
    "                               speeding.HYPERPARAMS.TARGET_SIZE,\n",
    "                               speeding.HYPERPARAMS.BATCH_SIZE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=speeding.HYPERPARAMS.LEARNING_RATE)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "        \n",
    "criterion = nn.MSELoss().cuda()\n",
    "\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', verbose=True)\n",
    "\n",
    "min_loss = checkpoint['min_loss']\n",
    "losses = {\n",
    "  'training': [],\n",
    "  'validation': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [201/400]\n",
      "--- Parsing track medium-race_12.csv\n",
      "--- Parsing track hard-race_22.csv\n",
      "--- Parsing track easy-race_03.csv\n",
      "--- Parsing track medium-race_13.csv\n",
      "--- Parsing track hard-race_21.csv\n",
      "--- Parsing track medium-race_10.csv\n",
      "--- Parsing track easy-race_00.csv\n",
      "--- Parsing track hard-race_20.csv\n",
      "--- Parsing track easy-race_01.csv\n",
      "--- Parsing track medium-race_11.csv\n",
      "--- Parsing track easy-race_02.csv\n",
      "--- Parsing track hard-race_23.csv\n",
      "--- TRAINING LOSS: 1.068375\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'split_checkpoints/speeding/latest_checkpoint.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-812a605dfaed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m           \u001b[0;34m'min_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmin_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m           \u001b[0;34m'optimizer'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       }, is_best)\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-001a11a3c99c>\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(state, is_best, filepath)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_best\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latest_checkpoint.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'split_checkpoints/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mCOMMAND\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_best\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'split_checkpoints/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mCOMMAND\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'best_checkpoint.tar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'split_checkpoints/speeding/latest_checkpoint.tar'"
     ]
    }
   ],
   "source": [
    "logging.info('Training {}...'.format(COMMAND))\n",
    "print('Training {}...'.format(COMMAND))\n",
    "for epoch in np.arange(checkpoint['epoch'], NUM_EPOCHS):\n",
    "    logging.info('Epoch [%d/%d]' %(epoch+1, NUM_EPOCHS))\n",
    "    print('Epoch [%d/%d]' %(epoch+1, NUM_EPOCHS))\n",
    "    \n",
    "    is_best = False\n",
    "\n",
    "    training_loss = train(TRAINING_DATA, model, criterion, optimizer)\n",
    "    logging.info('--- TRAINING LOSS: %f' % training_loss)\n",
    "    print('--- TRAINING LOSS: %f' % training_loss)\n",
    "\n",
    "#     validation_loss = validate(VALIDATION_DATA, model, criterion)\n",
    "#     logging.info('--- VALIDATION LOSS: %f' % validation_loss)\n",
    "#     print('--- VALIDATION LOSS: %f' % validation_loss)\n",
    "\n",
    "    if training_loss < min_loss:\n",
    "        logging.info('--- --- best model found so far: %f' % training_loss)\n",
    "        print('--- --- best model found so far: %f' % training_loss)\n",
    "        min_loss = training_loss\n",
    "        is_best = True\n",
    "\n",
    "    losses['training'].append(training_loss)\n",
    "#     losses['validation'].append(validation_loss)\n",
    "\n",
    "    save_checkpoint({\n",
    "          'epoch': epoch + 1,\n",
    "          'state_dict': model.state_dict(),\n",
    "          'min_loss': min_loss,\n",
    "          'optimizer' : optimizer.state_dict(),\n",
    "      }, is_best)\n",
    "\n",
    "    scheduler.step(training_loss)\n",
    "    logging.info('-------------------------------------------------------')\n",
    "    print('-------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dump losses to file\n",
    "with open('split_checkpoints/{}/losses_{}.pkl'.format(COMMAND, time.time()), 'wb') as file:\n",
    "    pickle.dump(losses, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
