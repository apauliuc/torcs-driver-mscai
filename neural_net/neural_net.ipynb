{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM implementation for TORCS driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import glob\n",
    "from sklearn.preprocessing import Imputer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, batch_size):\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        super(RNN, self).__init__()        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, output_size)        \n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self, x=None):\n",
    "        if x == None:\n",
    "            return (Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)),\n",
    "                    Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)))\n",
    "        else:\n",
    "            return (Variable(x[0].data),Variable(x[1].data))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden_out = self.lstm(x, self.hidden)\n",
    "        output = self.out(lstm_out.view(len(x), -1))\n",
    "        self.hidden = self.init_hidden(self.hidden_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 22\n",
    "HIDDEN_SIZE = 160\n",
    "NUM_LAYERS = 3\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "rnn = RNN(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, 3, BATCH_SIZE)\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = glob.glob('train_data/*.csv') + glob.glob('basic_data/*.csv')\n",
    "\n",
    "# Read all training sets\n",
    "training_sets = defaultdict(lambda: dict())\n",
    "\n",
    "for f in training_files:    \n",
    "    train_ds = pd.read_csv(f)\n",
    "    \n",
    "    X_train = train_ds.iloc[:, 3:].values\n",
    "    y_train = train_ds.iloc[:, :3].values\n",
    "    \n",
    "    imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    imputer = imputer.fit(X_train)\n",
    "    X_train = imputer.transform(X_train)\n",
    "    \n",
    "    X_train = torch.from_numpy(X_train).float()\n",
    "    y_train = torch.from_numpy(y_train).float()\n",
    "    \n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    \n",
    "    training_sets[f] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "  training set: f-speedway.csv\n",
      "  training set: alpine-1.csv\n",
      "    step: [100/170], loss: 0.1285\n",
      "  training set: aalborg.csv\n",
      "  training set: alpine_1_2_laps.csv\n",
      "    step: [100/282], loss: 0.0005\n",
      "    step: [200/282], loss: 0.0128\n",
      "  training set: forza_3_laps.csv\n",
      "    step: [100/378], loss: 0.0000\n",
      "    step: [200/378], loss: 0.0018\n",
      "    step: [300/378], loss: 0.0003\n",
      "  training set: cg_speedway_2_laps.csv\n",
      "  training set: cg_track2_2_laps.csv\n",
      "    step: [100/143], loss: 0.0006\n",
      "  training set: ruudskogen.csv\n",
      "    step: [100/147], loss: 0.0046\n",
      "  training set: brodenhach_2_laps.csv\n",
      "    step: [100/180], loss: 0.0022\n",
      "  training set: e_road_2_laps.csv\n",
      "    step: [100/147], loss: 0.0005\n",
      "Epoch [2/10]\n",
      "  training set: f-speedway.csv\n",
      "  training set: alpine-1.csv\n",
      "    step: [100/170], loss: 0.0814\n",
      "  training set: aalborg.csv\n",
      "  training set: alpine_1_2_laps.csv\n",
      "    step: [100/282], loss: 0.0003\n",
      "    step: [200/282], loss: 0.0106\n",
      "  training set: forza_3_laps.csv\n",
      "    step: [100/378], loss: 0.0000\n",
      "    step: [200/378], loss: 0.0015\n",
      "    step: [300/378], loss: 0.0003\n",
      "  training set: cg_speedway_2_laps.csv\n",
      "  training set: cg_track2_2_laps.csv\n",
      "    step: [100/143], loss: 0.0004\n",
      "  training set: ruudskogen.csv\n",
      "    step: [100/147], loss: 0.0024\n",
      "  training set: brodenhach_2_laps.csv\n",
      "    step: [100/180], loss: 0.0018\n",
      "  training set: e_road_2_laps.csv\n",
      "    step: [100/147], loss: 0.0001\n",
      "Epoch [3/10]\n",
      "  training set: f-speedway.csv\n",
      "  training set: alpine-1.csv\n",
      "    step: [100/170], loss: 0.0798\n",
      "  training set: aalborg.csv\n",
      "  training set: alpine_1_2_laps.csv\n",
      "    step: [100/282], loss: 0.0003\n",
      "    step: [200/282], loss: 0.0095\n",
      "  training set: forza_3_laps.csv\n",
      "    step: [100/378], loss: 0.0000\n",
      "    step: [200/378], loss: 0.0013\n",
      "    step: [300/378], loss: 0.0004\n",
      "  training set: cg_speedway_2_laps.csv\n",
      "  training set: cg_track2_2_laps.csv\n",
      "    step: [100/143], loss: 0.0005\n",
      "  training set: ruudskogen.csv\n",
      "    step: [100/147], loss: 0.0022\n",
      "  training set: brodenhach_2_laps.csv\n",
      "    step: [100/180], loss: 0.0023\n",
      "  training set: e_road_2_laps.csv\n",
      "    step: [100/147], loss: 0.0005\n",
      "Epoch [4/10]\n",
      "  training set: f-speedway.csv\n",
      "  training set: alpine-1.csv\n",
      "    step: [100/170], loss: 0.0588\n",
      "  training set: aalborg.csv\n",
      "  training set: alpine_1_2_laps.csv\n",
      "    step: [100/282], loss: 0.0003\n",
      "    step: [200/282], loss: 0.0156\n",
      "  training set: forza_3_laps.csv\n",
      "    step: [100/378], loss: 0.0000\n",
      "    step: [200/378], loss: 0.0019\n",
      "    step: [300/378], loss: 0.0003\n",
      "  training set: cg_speedway_2_laps.csv\n",
      "  training set: cg_track2_2_laps.csv\n",
      "    step: [100/143], loss: 0.0004\n",
      "  training set: ruudskogen.csv\n",
      "    step: [100/147], loss: 0.0031\n",
      "  training set: brodenhach_2_laps.csv\n",
      "    step: [100/180], loss: 0.0021\n",
      "  training set: e_road_2_laps.csv\n",
      "    step: [100/147], loss: 0.0002\n",
      "Epoch [5/10]\n",
      "  training set: f-speedway.csv\n",
      "  training set: alpine-1.csv\n",
      "    step: [100/170], loss: 0.1249\n",
      "  training set: aalborg.csv\n",
      "  training set: alpine_1_2_laps.csv\n",
      "    step: [100/282], loss: 0.0052\n",
      "    step: [200/282], loss: 0.0078\n",
      "  training set: forza_3_laps.csv\n",
      "    step: [100/378], loss: 0.0000\n",
      "    step: [200/378], loss: 0.0008\n",
      "    step: [300/378], loss: 0.0002\n",
      "  training set: cg_speedway_2_laps.csv\n",
      "  training set: cg_track2_2_laps.csv\n",
      "    step: [100/143], loss: 0.0003\n",
      "  training set: ruudskogen.csv\n",
      "    step: [100/147], loss: 0.0031\n",
      "  training set: brodenhach_2_laps.csv\n",
      "    step: [100/180], loss: 0.0018\n",
      "  training set: e_road_2_laps.csv\n",
      "    step: [100/147], loss: 0.0001\n",
      "Epoch [6/10]\n",
      "  training set: f-speedway.csv\n",
      "  training set: alpine-1.csv\n",
      "    step: [100/170], loss: 0.1095\n",
      "  training set: aalborg.csv\n",
      "  training set: alpine_1_2_laps.csv\n",
      "    step: [100/282], loss: 0.0008\n",
      "    step: [200/282], loss: 0.0146\n",
      "  training set: forza_3_laps.csv\n",
      "    step: [100/378], loss: 0.0000\n",
      "    step: [200/378], loss: 0.0015\n",
      "    step: [300/378], loss: 0.0002\n",
      "  training set: cg_speedway_2_laps.csv\n",
      "  training set: cg_track2_2_laps.csv\n",
      "    step: [100/143], loss: 0.0004\n",
      "  training set: ruudskogen.csv\n",
      "    step: [100/147], loss: 0.0025\n",
      "  training set: brodenhach_2_laps.csv\n",
      "    step: [100/180], loss: 0.0020\n",
      "  training set: e_road_2_laps.csv\n",
      "    step: [100/147], loss: 0.0004\n",
      "Epoch [7/10]\n",
      "  training set: f-speedway.csv\n",
      "  training set: alpine-1.csv\n",
      "    step: [100/170], loss: 0.0586\n",
      "  training set: aalborg.csv\n",
      "  training set: alpine_1_2_laps.csv\n",
      "    step: [100/282], loss: 0.0074\n",
      "    step: [200/282], loss: 0.0078\n",
      "  training set: forza_3_laps.csv\n",
      "    step: [100/378], loss: 0.0000\n",
      "    step: [200/378], loss: 0.0007\n",
      "    step: [300/378], loss: 0.0003\n",
      "  training set: cg_speedway_2_laps.csv\n",
      "  training set: cg_track2_2_laps.csv\n",
      "    step: [100/143], loss: 0.0003\n",
      "  training set: ruudskogen.csv\n",
      "    step: [100/147], loss: 0.0020\n",
      "  training set: brodenhach_2_laps.csv\n",
      "    step: [100/180], loss: 0.0016\n",
      "  training set: e_road_2_laps.csv\n",
      "    step: [100/147], loss: 0.0004\n",
      "Epoch [8/10]\n",
      "  training set: f-speedway.csv\n",
      "  training set: alpine-1.csv\n",
      "    step: [100/170], loss: 0.1215\n",
      "  training set: aalborg.csv\n",
      "  training set: alpine_1_2_laps.csv\n",
      "    step: [100/282], loss: 0.0043\n",
      "    step: [200/282], loss: 0.0077\n",
      "  training set: forza_3_laps.csv\n",
      "    step: [100/378], loss: 0.0000\n",
      "    step: [200/378], loss: 0.0013\n",
      "    step: [300/378], loss: 0.0002\n",
      "  training set: cg_speedway_2_laps.csv\n",
      "  training set: cg_track2_2_laps.csv\n",
      "    step: [100/143], loss: 0.0004\n",
      "  training set: ruudskogen.csv\n",
      "    step: [100/147], loss: 0.0021\n",
      "  training set: brodenhach_2_laps.csv\n",
      "    step: [100/180], loss: 0.0021\n",
      "  training set: e_road_2_laps.csv\n",
      "    step: [100/147], loss: 0.0001\n",
      "Epoch [9/10]\n",
      "  training set: f-speedway.csv\n",
      "  training set: alpine-1.csv\n",
      "    step: [100/170], loss: 0.0484\n",
      "  training set: aalborg.csv\n",
      "  training set: alpine_1_2_laps.csv\n",
      "    step: [100/282], loss: 0.0045\n",
      "    step: [200/282], loss: 0.0076\n",
      "  training set: forza_3_laps.csv\n",
      "    step: [100/378], loss: 0.0000\n",
      "    step: [200/378], loss: 0.0016\n",
      "    step: [300/378], loss: 0.0002\n",
      "  training set: cg_speedway_2_laps.csv\n",
      "  training set: cg_track2_2_laps.csv\n",
      "    step: [100/143], loss: 0.0004\n",
      "  training set: ruudskogen.csv\n",
      "    step: [100/147], loss: 0.0031\n",
      "  training set: brodenhach_2_laps.csv\n",
      "    step: [100/180], loss: 0.0023\n",
      "  training set: e_road_2_laps.csv\n",
      "    step: [100/147], loss: 0.0004\n",
      "Epoch [10/10]\n",
      "  training set: f-speedway.csv\n",
      "  training set: alpine-1.csv\n",
      "    step: [100/170], loss: 0.0685\n",
      "  training set: aalborg.csv\n",
      "  training set: alpine_1_2_laps.csv\n",
      "    step: [100/282], loss: 0.0052\n",
      "    step: [200/282], loss: 0.0074\n",
      "  training set: forza_3_laps.csv\n",
      "    step: [100/378], loss: 0.0000\n",
      "    step: [200/378], loss: 0.0011\n",
      "    step: [300/378], loss: 0.0002\n",
      "  training set: cg_speedway_2_laps.csv\n",
      "  training set: cg_track2_2_laps.csv\n",
      "    step: [100/143], loss: 0.0003\n",
      "  training set: ruudskogen.csv\n",
      "    step: [100/147], loss: 0.0022\n",
      "  training set: brodenhach_2_laps.csv\n",
      "    step: [100/180], loss: 0.0021\n",
      "  training set: e_road_2_laps.csv\n",
      "    step: [100/147], loss: 0.0005\n",
      "Training done\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('Epoch [%d/%d]' %(epoch+1, NUM_EPOCHS))\n",
    "    \n",
    "    for f in training_files:\n",
    "        print('  training set: %s' %(f[f.find('/')+1:]))        \n",
    "        train_loader = DataLoader(dataset=training_sets[f], batch_size=BATCH_SIZE, shuffle=False)    \n",
    "        rnn.init_hidden()\n",
    "\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            if (len(X) != BATCH_SIZE):\n",
    "                continue\n",
    "\n",
    "            data = Variable(X.view(-1, 1, INPUT_SIZE))\n",
    "            target = Variable(y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction = rnn(data)\n",
    "            loss = criterion(prediction, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % BATCH_SIZE == 0:\n",
    "                print('    step: [%d/%d], loss: %.4f'\n",
    "                      %(i+1, len(training_sets[f].target_tensor)//BATCH_SIZE, loss.data[0]))\n",
    "\n",
    "print('Training done')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn.state_dict(), 'rnn_params.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(rnn, 'whole_net.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in training_files:\n",
    "#     # Train the network for each training session\n",
    "#     print('Training file: %s' %(file))\n",
    "    \n",
    "#     train_ds = pd.read_csv(file)\n",
    "#     X_train = train_ds.iloc[:, 3:].values\n",
    "#     y_train = train_ds.iloc[:, :3].values\n",
    "    \n",
    "#     imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "#     imputer = imputer.fit(X_train)\n",
    "#     X_train = imputer.transform(X_train)\n",
    "    \n",
    "#     X_train = torch.from_numpy(X_train).float()\n",
    "#     y_train = torch.from_numpy(y_train).float()\n",
    "    \n",
    "#     dataset = TensorDataset(X_train, y_train)\n",
    "    \n",
    "#     train_loader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "#     rnn.init_hidden()\n",
    "    \n",
    "#     for epoch in range(NUM_EPOCHS):\n",
    "#         for i, (X, y) in enumerate(train_loader):\n",
    "#             if (len(X) != BATCH_SIZE):\n",
    "#                 continue\n",
    "            \n",
    "#             data = Variable(X.view(-1, 1, INPUT_SIZE))\n",
    "#             target = Variable(y)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             prediction = rnn(data)\n",
    "#             loss = criterion(prediction, target)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             if (i+1) % 30 == 0:\n",
    "#                 print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "#                       %(epoch+1, NUM_EPOCHS, i+1, len(X_train)//BATCH_SIZE, loss.data[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
