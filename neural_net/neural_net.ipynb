{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM implementation for TORCS driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import Imputer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, batch_size):\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        super(RNN_LSTM, self).__init__()        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, output_size)        \n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self, x=None):\n",
    "        if x == None:\n",
    "            return (Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)),\n",
    "                    Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)))\n",
    "        else:\n",
    "            return (Variable(x[0].data),Variable(x[1].data))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, self.hidden_out = self.lstm(x, self.hidden)\n",
    "        output = self.out(lstm_out.view(len(x), -1))\n",
    "        self.hidden = self.init_hidden(self.hidden_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "\n",
    "The LSTM has 28 inputs, 3 hidden layers with 28 hidden units, and an output layer with 3 nodes. The batch size is set to 100, while the learning rate is variable along the 500 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 28\n",
    "HIDDEN_SIZE = 28\n",
    "NUM_LAYERS = 3\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 500\n",
    "# LRS = np.concatenate((np.arange(0.9, 0.1, -0.1),\n",
    "#                       [0.15, 0.1, 0.05, 0.01, 0.0075, 0.005, 0.0025, 0.001, 0.00075, 0.0005, 0.00025, 0.0001]))\n",
    "LRS = np.concatenate((np.arange(0.9, 0, -0.2),\n",
    "                      [0.01, 0.005, 0.001, 0.0005, 0.0001]))\n",
    "\n",
    "lstm_nn = RNN_LSTM(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, 3, BATCH_SIZE)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 3.0, 4.0]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[normalize(i, 0, 1) for i in list((1, 2, 3, 4))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, min, max):\n",
    "    '''Normalization of a vector to values between [0, 1]'''\n",
    "    return (x - min)/(max-min)\n",
    "\n",
    "# Get training filenames\n",
    "training_files = glob.glob('')\n",
    "\n",
    "# Search min and max values for specific parameters\n",
    "maxSpeedX = -10000\n",
    "minSpeedX = 10000\n",
    "maxSpeedY = -10000\n",
    "minSpeedY = 10000\n",
    "maxRPM = 0\n",
    "maxWheelSpin = 0\n",
    "\n",
    "for f in training_files:\n",
    "    train_ds = pd.read_csv(f)    \n",
    "    X = train_ds.iloc[:, :-4].values\n",
    "    \n",
    "    if X[0].max() > maxSpeedX:\n",
    "        maxSpeedX = X[0].max()\n",
    "    if X[0].min() < minSpeedX:\n",
    "        minSpeedX = X[0].min()\n",
    "        \n",
    "    if X[1].max() > maxSpeedY:\n",
    "        maxSpeedY = X[1].max()\n",
    "    if X[1].min() < minSpeedY:\n",
    "        minSpeedY = X[1].min()    \n",
    "    \n",
    "    if X[4].max() > maxRPM:\n",
    "        maxRPM = X[4].max()\n",
    "    \n",
    "    if X[5].max() > maxWheelSpin:\n",
    "        maxWheelSpin = X[5].max()\n",
    "    if X[6].max() > maxWheelSpin:\n",
    "        maxWheelSpin = X[6].max()\n",
    "    if X[7].max() > maxWheelSpin:\n",
    "        maxWheelSpin = X[7].max()\n",
    "    if X[8].max() > maxWheelSpin:\n",
    "        maxWheelSpin = X[8].max()\n",
    "\n",
    "# Save their values\n",
    "param_dict = {\n",
    "    'maxSpeedX':maxSpeedX,\n",
    "    'minSpeedX':minSpeedX,\n",
    "    'maxSpeedY':maxSpeedY,\n",
    "    'minSpeedY':minSpeedY,\n",
    "    'maxRPM':maxRPM,\n",
    "    'maxWheelSpin':maxWheelSpin\n",
    "}\n",
    "with open('norm_parameters.pickle', 'wb') as handle:\n",
    "    pickle.dump(param_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Read all training sets\n",
    "training_sets = defaultdict(lambda: dict())\n",
    "\n",
    "for f in training_files:\n",
    "    # read dataset\n",
    "    train_ds = pd.read_csv(f, header=False)\n",
    "    \n",
    "    X = train_ds.iloc[:, :-4].values\n",
    "    y = train_ds.iloc[:, -4:].values\n",
    "    \n",
    "    # fill missing values with mean\n",
    "    imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    imputer = imputer.fit(X)\n",
    "    X = imputer.transform(X)\n",
    "    \n",
    "    # normalize all values for interval [0, 1]    \n",
    "    X[0] = normalize(X[0], minSpeedX, maxSpeedX)  # speedX = range(search min, search max)\n",
    "    X[1] = normalize(X[1], minSpeedY, maxSpeedY)  # speedY = range(search min, search max)\n",
    "    X[2] = normalize(X[2], -180, 180)  # angle = range(-180, 180)\n",
    "    X[3] = normalize(X[3], -1, 6)  # currentGear = range(-1, 6)\n",
    "    X[4] = normalize(X[4], 0, maxRPM) . # RPM = range(0, search max)\n",
    "    for i in np.arange(5, 9):\n",
    "        X[i] = normalize(X[i], 0, maxWheelSpin)  # *wheelSpin = range(0, search max)\n",
    "    for i in np.arange(9, 28):\n",
    "        X[i] = normalize(X[i], 0, 200)  # *sensorValues = range(0, 200)\n",
    "    y[0] = normalize(y[0], -1, 6)  # gear = range(-1, 6)\n",
    "    y[1] = normalize(y[1], -1, 1)  # steering = range(-1, 1)\n",
    "    # for acceleration and break, compute their difference and normalize it\n",
    "    accel_brake = y[2] - y[3]\n",
    "    y[2] = normalize(accel_brake, -1, 1)  # accelerate-brake = range(-1, 1)\n",
    "    y = np.delete(y, 3, axis=1)\n",
    "    \n",
    "    # Create TensorDataset from FloatTensors and save to dictionary\n",
    "    X_train = torch.from_numpy(X).float()\n",
    "    y_train = torch.from_numpy(y).float()\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    training_sets[f] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "0.7\n",
      "0.5\n",
      "0.3\n",
      "0.1\n",
      "0.01\n",
      "0.005\n",
      "0.001\n",
      "0.0005\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "lridx = -1\n",
    "\n",
    "for epoch in np.arange(NUM_EPOCHS):\n",
    "    print('Epoch [%d/%d]' %(epoch+1, NUM_EPOCHS))\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        lridx += 1\n",
    "        optimizer = torch.optim.Adam(rnn.parameters(), lr=LRS[lridx])\n",
    "    \n",
    "    for f in training_files:\n",
    "#         print('  training set: %s' %(f[f.find('/')+1:]))        \n",
    "        train_loader = DataLoader(dataset=training_sets[f], batch_size=BATCH_SIZE, shuffle=False)    \n",
    "        lstm_nn.init_hidden()\n",
    "\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            if (len(X) != BATCH_SIZE):\n",
    "                continue\n",
    "\n",
    "            data = Variable(X.view(-1, 1, INPUT_SIZE))\n",
    "            target = Variable(y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction = lstm_nn(data)\n",
    "            loss = criterion(prediction, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % BATCH_SIZE == 0:\n",
    "                print('    step: [%d/%d], loss: %.4f'\n",
    "                      %(i+1, len(training_sets[f].target_tensor)//BATCH_SIZE, loss.data[0]))\n",
    "\n",
    "print('Training done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model parameters\n",
    "\n",
    "Parameters will be used by the driver to get a command to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn.state_dict(), 'rnn_params.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(rnn, 'whole_net.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in training_files:\n",
    "#     # Train the network for each training session\n",
    "#     print('Training file: %s' %(file))\n",
    "    \n",
    "#     train_ds = pd.read_csv(file)\n",
    "#     X_train = train_ds.iloc[:, 3:].values\n",
    "#     y_train = train_ds.iloc[:, :3].values\n",
    "    \n",
    "#     imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "#     imputer = imputer.fit(X_train)\n",
    "#     X_train = imputer.transform(X_train)\n",
    "    \n",
    "#     X_train = torch.from_numpy(X_train).float()\n",
    "#     y_train = torch.from_numpy(y_train).float()\n",
    "    \n",
    "#     dataset = TensorDataset(X_train, y_train)\n",
    "    \n",
    "#     train_loader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "#     rnn.init_hidden()\n",
    "    \n",
    "#     for epoch in range(NUM_EPOCHS):\n",
    "#         for i, (X, y) in enumerate(train_loader):\n",
    "#             if (len(X) != BATCH_SIZE):\n",
    "#                 continue\n",
    "            \n",
    "#             data = Variable(X.view(-1, 1, INPUT_SIZE))\n",
    "#             target = Variable(y)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             prediction = rnn(data)\n",
    "#             loss = criterion(prediction, target)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             if (i+1) % 30 == 0:\n",
    "#                 print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "#                       %(epoch+1, NUM_EPOCHS, i+1, len(X_train)//BATCH_SIZE, loss.data[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
