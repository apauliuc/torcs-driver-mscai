{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM implementation for TORCS driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import Imputer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, batch_size):\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        super(RNN_LSTM, self).__init__()        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, output_size)        \n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self, x=None):\n",
    "        if x == None:\n",
    "            return (Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)),\n",
    "                    Variable(torch.zeros(self.num_layers, self.batch_size, self.hidden_size)))\n",
    "        else:\n",
    "            return (Variable(x[0].data),Variable(x[1].data))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, hidden_out = self.lstm(x, self.hidden)\n",
    "        output = self.out(lstm_out.view(len(x), -1))\n",
    "        self.hidden = self.init_hidden(hidden_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "\n",
    "The LSTM has 28 inputs, 3 hidden layers with 28 hidden units, and an output layer with 3 nodes. The batch size is set to 100, while the learning rate is variable along the 500 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_LR = {\n",
    "    0.9: 10,\n",
    "    0.7: 11,\n",
    "    0.5: 15,\n",
    "    0.3: 20,\n",
    "    0.1: 25,\n",
    "    0.01: 30,\n",
    "    0.005: 40,\n",
    "    0.001: 50,\n",
    "    0.0005: 50,\n",
    "    0.0001: 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 28\n",
    "HIDDEN_SIZE = 28\n",
    "NUM_LAYERS = 3\n",
    "BATCH_SIZE = 50\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.0001\n",
    "# LRS = np.concatenate((np.arange(0.9, 0.1, -0.1),\n",
    "#                       [0.15, 0.1, 0.05, 0.01, 0.0075, 0.005, 0.0025, 0.001, 0.00075, 0.0005, 0.00025, 0.0001]))\n",
    "LRS = np.concatenate((np.arange(0.9, 0, -0.2),\n",
    "                      [0.01, 0.005, 0.001, 0.0005, 0.0001]))\n",
    "\n",
    "lstm_nn = RNN_LSTM(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, 3, BATCH_SIZE)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, mmin, mmax):\n",
    "    return (x - mmin)/(mmax-mmin)\n",
    "\n",
    "# Get training filenames\n",
    "training_files = glob.glob('train_data/*.csv')\n",
    "\n",
    "# Search min and max values for specific parameters\n",
    "maxSpeedX = -10000\n",
    "minSpeedX = 10000\n",
    "maxSpeedY = -10000\n",
    "minSpeedY = 10000\n",
    "maxRPM = 0\n",
    "maxWheelSpin = 0\n",
    "\n",
    "for f in training_files:\n",
    "    train_ds = pd.read_csv(f)    \n",
    "    X = train_ds.iloc[:, :-4].values\n",
    "    \n",
    "    if X[0].max() > maxSpeedX:\n",
    "        maxSpeedX = X[0].max()\n",
    "    if X[0].min() < minSpeedX:\n",
    "        minSpeedX = X[0].min()\n",
    "        \n",
    "    if X[1].max() > maxSpeedY:\n",
    "        maxSpeedY = X[1].max()\n",
    "    if X[1].min() < minSpeedY:\n",
    "        minSpeedY = X[1].min()    \n",
    "    \n",
    "    if X[4].max() > maxRPM:\n",
    "        maxRPM = X[4].max()\n",
    "    \n",
    "    if X[5].max() > maxWheelSpin:\n",
    "        maxWheelSpin = X[5].max()\n",
    "    if X[6].max() > maxWheelSpin:\n",
    "        maxWheelSpin = X[6].max()\n",
    "    if X[7].max() > maxWheelSpin:\n",
    "        maxWheelSpin = X[7].max()\n",
    "    if X[8].max() > maxWheelSpin:\n",
    "        maxWheelSpin = X[8].max()\n",
    "\n",
    "# Save their values\n",
    "param_dict = {\n",
    "    'maxSpeedX':maxSpeedX,\n",
    "    'minSpeedX':minSpeedX,\n",
    "    'maxSpeedY':maxSpeedY,\n",
    "    'minSpeedY':minSpeedY,\n",
    "    'maxRPM':maxRPM,\n",
    "    'maxWheelSpin':maxWheelSpin\n",
    "}\n",
    "with open('norm_parameters.pickle', 'wb') as handle:\n",
    "    pickle.dump(param_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Read all training sets\n",
    "training_sets = defaultdict(lambda: dict())\n",
    "\n",
    "for f in training_files:\n",
    "    # read dataset\n",
    "    train_ds = pd.read_csv(f, header=None)\n",
    "    \n",
    "    X = train_ds.iloc[:, :-4].values\n",
    "    y = train_ds.iloc[:, -4:].values\n",
    "    \n",
    "    # fill missing values with mean\n",
    "    imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    imputer = imputer.fit(X)\n",
    "    X = imputer.transform(X)\n",
    "    \n",
    "    # normalize all values for interval [0, 1]    \n",
    "    X[0] = normalize(X[0], minSpeedX, maxSpeedX)  # speedX = range(search min, search max)\n",
    "    X[1] = normalize(X[1], minSpeedY, maxSpeedY)  # speedY = range(search min, search max)\n",
    "    X[2] = normalize(X[2], -180, 180)  # angle = range(-180, 180)\n",
    "    X[3] = normalize(X[3], -1, 6)  # currentGear = range(-1, 6)\n",
    "    X[4] = normalize(X[4], 0, maxRPM)  # RPM = range(0, search max)\n",
    "    for i in np.arange(5, 9):\n",
    "        X[i] = normalize(X[i], 0, maxWheelSpin)  # *wheelSpin = range(0, search max)\n",
    "    for i in np.arange(9, 28):\n",
    "        X[i] = normalize(X[i], 0, 200)  # *sensorValues = range(0, 200)\n",
    "    y[0] = normalize(y[0], -1, 6)  # gear = range(-1, 6)\n",
    "    y[1] = normalize(y[1], -1, 1)  # steering = range(-1, 1)\n",
    "    # for acceleration and break, compute their difference and normalize it\n",
    "    accel_brake = y[2] - y[3]\n",
    "    y[2] = normalize(accel_brake, -1, 1)  # accelerate-brake = range(-1, 1)\n",
    "    y = np.delete(y, 3, axis=1)\n",
    "    \n",
    "    # Create TensorDataset from FloatTensors and save to dictionary\n",
    "    X_train = torch.from_numpy(X).float()\n",
    "    y_train = torch.from_numpy(y).float()\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    training_sets[f] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.9000\n",
      "  epoch: [1/10], step: [100/116], loss: 0.9781\n",
      "  epoch: [1/10], step: [100/231], loss: 2.4053\n",
      "  epoch: [1/10], step: [200/231], loss: 0.4660\n",
      "  epoch: [1/10], step: [100/118], loss: 0.6651\n",
      "  epoch: [2/10], step: [100/116], loss: 0.9013\n",
      "  epoch: [2/10], step: [100/231], loss: 2.4730\n",
      "  epoch: [2/10], step: [200/231], loss: 0.5272\n",
      "  epoch: [2/10], step: [100/118], loss: 0.7848\n",
      "  epoch: [3/10], step: [100/116], loss: 0.9126\n",
      "  epoch: [3/10], step: [100/231], loss: 2.4304\n",
      "  epoch: [3/10], step: [200/231], loss: 0.5410\n",
      "  epoch: [3/10], step: [100/118], loss: 0.8292\n",
      "  epoch: [4/10], step: [100/116], loss: 0.9217\n",
      "  epoch: [4/10], step: [100/231], loss: 2.4125\n",
      "  epoch: [4/10], step: [200/231], loss: 0.5449\n",
      "  epoch: [4/10], step: [100/118], loss: 0.8476\n",
      "  epoch: [5/10], step: [100/116], loss: 0.9255\n",
      "  epoch: [5/10], step: [100/231], loss: 2.4032\n",
      "  epoch: [5/10], step: [200/231], loss: 0.5433\n",
      "  epoch: [5/10], step: [100/118], loss: 0.8553\n",
      "  epoch: [6/10], step: [100/116], loss: 0.9271\n",
      "  epoch: [6/10], step: [100/231], loss: 2.3979\n",
      "  epoch: [6/10], step: [200/231], loss: 0.5417\n",
      "  epoch: [6/10], step: [100/118], loss: 0.8588\n",
      "  epoch: [7/10], step: [100/116], loss: 0.9278\n",
      "  epoch: [7/10], step: [100/231], loss: 2.3950\n",
      "  epoch: [7/10], step: [200/231], loss: 0.5406\n",
      "  epoch: [7/10], step: [100/118], loss: 0.8605\n",
      "  epoch: [8/10], step: [100/116], loss: 0.9282\n",
      "  epoch: [8/10], step: [100/231], loss: 2.3933\n",
      "  epoch: [8/10], step: [200/231], loss: 0.5401\n",
      "  epoch: [8/10], step: [100/118], loss: 0.8613\n",
      "  epoch: [9/10], step: [100/116], loss: 0.9283\n",
      "  epoch: [9/10], step: [100/231], loss: 2.3924\n",
      "  epoch: [9/10], step: [200/231], loss: 0.5397\n",
      "  epoch: [9/10], step: [100/118], loss: 0.8617\n",
      "  epoch: [10/10], step: [100/116], loss: 0.9284\n",
      "  epoch: [10/10], step: [100/231], loss: 2.3919\n",
      "  epoch: [10/10], step: [200/231], loss: 0.5396\n",
      "  epoch: [10/10], step: [100/118], loss: 0.8619\n",
      "learning rate 0.7000\n",
      "  epoch: [1/11], step: [100/116], loss: 0.9189\n",
      "  epoch: [1/11], step: [100/231], loss: 1.8693\n",
      "  epoch: [1/11], step: [200/231], loss: 0.4897\n",
      "  epoch: [1/11], step: [100/118], loss: 0.6098\n",
      "  epoch: [2/11], step: [100/116], loss: 0.9470\n",
      "  epoch: [2/11], step: [100/231], loss: 1.7316\n",
      "  epoch: [2/11], step: [200/231], loss: 0.4841\n",
      "  epoch: [2/11], step: [100/118], loss: 0.6131\n",
      "  epoch: [3/11], step: [100/116], loss: 0.9549\n",
      "  epoch: [3/11], step: [100/231], loss: 1.7003\n",
      "  epoch: [3/11], step: [200/231], loss: 0.4844\n",
      "  epoch: [3/11], step: [100/118], loss: 0.6151\n",
      "  epoch: [4/11], step: [100/116], loss: 0.9577\n",
      "  epoch: [4/11], step: [100/231], loss: 1.6883\n",
      "  epoch: [4/11], step: [200/231], loss: 0.4847\n",
      "  epoch: [4/11], step: [100/118], loss: 0.6161\n",
      "  epoch: [5/11], step: [100/116], loss: 0.9589\n",
      "  epoch: [5/11], step: [100/231], loss: 1.6827\n",
      "  epoch: [5/11], step: [200/231], loss: 0.4850\n",
      "  epoch: [5/11], step: [100/118], loss: 0.6166\n",
      "  epoch: [6/11], step: [100/116], loss: 0.9595\n",
      "  epoch: [6/11], step: [100/231], loss: 1.6799\n",
      "  epoch: [6/11], step: [200/231], loss: 0.4851\n",
      "  epoch: [6/11], step: [100/118], loss: 0.6169\n",
      "  epoch: [7/11], step: [100/116], loss: 0.9598\n",
      "  epoch: [7/11], step: [100/231], loss: 1.6785\n",
      "  epoch: [7/11], step: [200/231], loss: 0.4852\n",
      "  epoch: [7/11], step: [100/118], loss: 0.6171\n",
      "  epoch: [8/11], step: [100/116], loss: 0.9600\n",
      "  epoch: [8/11], step: [100/231], loss: 1.6777\n",
      "  epoch: [8/11], step: [200/231], loss: 0.4852\n",
      "  epoch: [8/11], step: [100/118], loss: 0.6171\n",
      "  epoch: [9/11], step: [100/116], loss: 0.9600\n",
      "  epoch: [9/11], step: [100/231], loss: 1.6773\n",
      "  epoch: [9/11], step: [200/231], loss: 0.4852\n",
      "  epoch: [9/11], step: [100/118], loss: 0.6172\n",
      "  epoch: [10/11], step: [100/116], loss: 0.9601\n",
      "  epoch: [10/11], step: [100/231], loss: 1.6770\n",
      "  epoch: [10/11], step: [200/231], loss: 0.4852\n",
      "  epoch: [10/11], step: [100/118], loss: 0.6172\n",
      "  epoch: [11/11], step: [100/116], loss: 0.9601\n",
      "  epoch: [11/11], step: [100/231], loss: 1.6769\n",
      "  epoch: [11/11], step: [200/231], loss: 0.4853\n",
      "  epoch: [11/11], step: [100/118], loss: 0.6172\n",
      "learning rate 0.5000\n",
      "  epoch: [1/15], step: [100/116], loss: 0.9727\n",
      "  epoch: [1/15], step: [100/231], loss: 1.4751\n",
      "  epoch: [1/15], step: [200/231], loss: 0.2568\n",
      "  epoch: [1/15], step: [100/118], loss: 0.5880\n",
      "  epoch: [2/15], step: [100/116], loss: 1.1145\n",
      "  epoch: [2/15], step: [100/231], loss: 1.4844\n",
      "  epoch: [2/15], step: [200/231], loss: 0.2487\n",
      "  epoch: [2/15], step: [100/118], loss: 0.5826\n",
      "  epoch: [3/15], step: [100/116], loss: 1.1266\n",
      "  epoch: [3/15], step: [100/231], loss: 1.4847\n",
      "  epoch: [3/15], step: [200/231], loss: 0.2477\n",
      "  epoch: [3/15], step: [100/118], loss: 0.5812\n",
      "  epoch: [4/15], step: [100/116], loss: 1.1306\n",
      "  epoch: [4/15], step: [100/231], loss: 1.4846\n",
      "  epoch: [4/15], step: [200/231], loss: 0.2475\n",
      "  epoch: [4/15], step: [100/118], loss: 0.5806\n",
      "  epoch: [5/15], step: [100/116], loss: 1.1323\n",
      "  epoch: [5/15], step: [100/231], loss: 1.4845\n",
      "  epoch: [5/15], step: [200/231], loss: 0.2475\n",
      "  epoch: [5/15], step: [100/118], loss: 0.5804\n",
      "  epoch: [6/15], step: [100/116], loss: 1.1332\n",
      "  epoch: [6/15], step: [100/231], loss: 1.4844\n",
      "  epoch: [6/15], step: [200/231], loss: 0.2475\n",
      "  epoch: [6/15], step: [100/118], loss: 0.5803\n",
      "  epoch: [7/15], step: [100/116], loss: 1.1336\n",
      "  epoch: [7/15], step: [100/231], loss: 1.4844\n",
      "  epoch: [7/15], step: [200/231], loss: 0.2475\n",
      "  epoch: [7/15], step: [100/118], loss: 0.5802\n",
      "  epoch: [8/15], step: [100/116], loss: 1.1338\n",
      "  epoch: [8/15], step: [100/231], loss: 1.4843\n",
      "  epoch: [8/15], step: [200/231], loss: 0.2475\n",
      "  epoch: [8/15], step: [100/118], loss: 0.5802\n",
      "  epoch: [9/15], step: [100/116], loss: 1.1339\n",
      "  epoch: [9/15], step: [100/231], loss: 1.4843\n",
      "  epoch: [9/15], step: [200/231], loss: 0.2475\n",
      "  epoch: [9/15], step: [100/118], loss: 0.5802\n",
      "  epoch: [10/15], step: [100/116], loss: 1.1340\n",
      "  epoch: [10/15], step: [100/231], loss: 1.4843\n",
      "  epoch: [10/15], step: [200/231], loss: 0.2475\n",
      "  epoch: [10/15], step: [100/118], loss: 0.5802\n",
      "  epoch: [11/15], step: [100/116], loss: 1.1340\n",
      "  epoch: [11/15], step: [100/231], loss: 1.4843\n",
      "  epoch: [11/15], step: [200/231], loss: 0.2475\n",
      "  epoch: [11/15], step: [100/118], loss: 0.5802\n",
      "  epoch: [12/15], step: [100/116], loss: 1.1340\n",
      "  epoch: [12/15], step: [100/231], loss: 1.4843\n",
      "  epoch: [12/15], step: [200/231], loss: 0.2475\n",
      "  epoch: [12/15], step: [100/118], loss: 0.5802\n",
      "  epoch: [13/15], step: [100/116], loss: 1.1340\n",
      "  epoch: [13/15], step: [100/231], loss: 1.4843\n",
      "  epoch: [13/15], step: [200/231], loss: 0.2475\n",
      "  epoch: [13/15], step: [100/118], loss: 0.5802\n",
      "  epoch: [14/15], step: [100/116], loss: 1.1340\n",
      "  epoch: [14/15], step: [100/231], loss: 1.4843\n",
      "  epoch: [14/15], step: [200/231], loss: 0.2475\n",
      "  epoch: [14/15], step: [100/118], loss: 0.5802\n",
      "  epoch: [15/15], step: [100/116], loss: 1.1340\n",
      "  epoch: [15/15], step: [100/231], loss: 1.4843\n",
      "  epoch: [15/15], step: [200/231], loss: 0.2475\n",
      "  epoch: [15/15], step: [100/118], loss: 0.5802\n",
      "learning rate 0.3000\n",
      "  epoch: [1/20], step: [100/116], loss: 1.2243\n",
      "  epoch: [1/20], step: [100/231], loss: 1.2953\n",
      "  epoch: [1/20], step: [200/231], loss: 0.2954\n",
      "  epoch: [1/20], step: [100/118], loss: 0.6224\n",
      "  epoch: [2/20], step: [100/116], loss: 1.2413\n",
      "  epoch: [2/20], step: [100/231], loss: 1.2727\n",
      "  epoch: [2/20], step: [200/231], loss: 0.2848\n",
      "  epoch: [2/20], step: [100/118], loss: 0.6221\n",
      "  epoch: [3/20], step: [100/116], loss: 1.2373\n",
      "  epoch: [3/20], step: [100/231], loss: 1.2703\n",
      "  epoch: [3/20], step: [200/231], loss: 0.2825\n",
      "  epoch: [3/20], step: [100/118], loss: 0.6219\n",
      "  epoch: [4/20], step: [100/116], loss: 1.2358\n",
      "  epoch: [4/20], step: [100/231], loss: 1.2695\n",
      "  epoch: [4/20], step: [200/231], loss: 0.2817\n",
      "  epoch: [4/20], step: [100/118], loss: 0.6219\n",
      "  epoch: [5/20], step: [100/116], loss: 1.2352\n",
      "  epoch: [5/20], step: [100/231], loss: 1.2692\n",
      "  epoch: [5/20], step: [200/231], loss: 0.2813\n",
      "  epoch: [5/20], step: [100/118], loss: 0.6218\n",
      "  epoch: [6/20], step: [100/116], loss: 1.2349\n",
      "  epoch: [6/20], step: [100/231], loss: 1.2691\n",
      "  epoch: [6/20], step: [200/231], loss: 0.2811\n",
      "  epoch: [6/20], step: [100/118], loss: 0.6218\n",
      "  epoch: [7/20], step: [100/116], loss: 1.2347\n",
      "  epoch: [7/20], step: [100/231], loss: 1.2690\n",
      "  epoch: [7/20], step: [200/231], loss: 0.2810\n",
      "  epoch: [7/20], step: [100/118], loss: 0.6218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch: [8/20], step: [100/116], loss: 1.2347\n",
      "  epoch: [8/20], step: [100/231], loss: 1.2690\n",
      "  epoch: [8/20], step: [200/231], loss: 0.2810\n",
      "  epoch: [8/20], step: [100/118], loss: 0.6218\n",
      "  epoch: [9/20], step: [100/116], loss: 1.2346\n",
      "  epoch: [9/20], step: [100/231], loss: 1.2690\n",
      "  epoch: [9/20], step: [200/231], loss: 0.2809\n",
      "  epoch: [9/20], step: [100/118], loss: 0.6218\n",
      "  epoch: [10/20], step: [100/116], loss: 1.2346\n",
      "  epoch: [10/20], step: [100/231], loss: 1.2690\n",
      "  epoch: [10/20], step: [200/231], loss: 0.2809\n",
      "  epoch: [10/20], step: [100/118], loss: 0.6218\n",
      "  epoch: [11/20], step: [100/116], loss: 1.2346\n",
      "  epoch: [11/20], step: [100/231], loss: 1.2690\n",
      "  epoch: [11/20], step: [200/231], loss: 0.2809\n",
      "  epoch: [11/20], step: [100/118], loss: 0.6218\n",
      "  epoch: [12/20], step: [100/116], loss: 1.2346\n",
      "  epoch: [12/20], step: [100/231], loss: 1.2690\n",
      "  epoch: [12/20], step: [200/231], loss: 0.2809\n",
      "  epoch: [12/20], step: [100/118], loss: 0.6218\n",
      "  epoch: [13/20], step: [100/116], loss: 1.2346\n",
      "  epoch: [13/20], step: [100/231], loss: 1.2690\n",
      "  epoch: [13/20], step: [200/231], loss: 0.2809\n",
      "  epoch: [13/20], step: [100/118], loss: 0.6218\n",
      "  epoch: [14/20], step: [100/116], loss: 1.2346\n",
      "  epoch: [14/20], step: [100/231], loss: 1.2690\n",
      "  epoch: [14/20], step: [200/231], loss: 0.2809\n",
      "  epoch: [14/20], step: [100/118], loss: 0.6218\n",
      "  epoch: [15/20], step: [100/116], loss: 1.2346\n",
      "  epoch: [15/20], step: [100/231], loss: 1.2690\n",
      "  epoch: [15/20], step: [200/231], loss: 0.2809\n",
      "  epoch: [15/20], step: [100/118], loss: 0.6218\n",
      "  epoch: [16/20], step: [100/116], loss: 1.2346\n",
      "  epoch: [16/20], step: [100/231], loss: 1.2690\n",
      "  epoch: [16/20], step: [200/231], loss: 0.2809\n",
      "  epoch: [16/20], step: [100/118], loss: 0.6218\n",
      "  epoch: [17/20], step: [100/116], loss: 1.2346\n",
      "  epoch: [17/20], step: [100/231], loss: 1.2690\n",
      "  epoch: [17/20], step: [200/231], loss: 0.2809\n",
      "  epoch: [17/20], step: [100/118], loss: 0.6218\n",
      "  epoch: [18/20], step: [100/116], loss: 1.2346\n",
      "  epoch: [18/20], step: [100/231], loss: 1.2690\n",
      "  epoch: [18/20], step: [200/231], loss: 0.2809\n",
      "  epoch: [18/20], step: [100/118], loss: 0.6218\n",
      "  epoch: [19/20], step: [100/116], loss: 1.2346\n",
      "  epoch: [19/20], step: [100/231], loss: 1.2690\n",
      "  epoch: [19/20], step: [200/231], loss: 0.2809\n",
      "  epoch: [19/20], step: [100/118], loss: 0.6218\n",
      "  epoch: [20/20], step: [100/116], loss: 1.2346\n",
      "  epoch: [20/20], step: [100/231], loss: 1.2690\n",
      "  epoch: [20/20], step: [200/231], loss: 0.2809\n",
      "  epoch: [20/20], step: [100/118], loss: 0.6218\n",
      "learning rate 0.1000\n",
      "  epoch: [1/25], step: [100/116], loss: 0.9753\n",
      "  epoch: [1/25], step: [100/231], loss: 1.3287\n",
      "  epoch: [1/25], step: [200/231], loss: 0.2168\n",
      "  epoch: [1/25], step: [100/118], loss: 0.4378\n",
      "  epoch: [2/25], step: [100/116], loss: 0.9618\n",
      "  epoch: [2/25], step: [100/231], loss: 1.3308\n",
      "  epoch: [2/25], step: [200/231], loss: 0.2248\n",
      "  epoch: [2/25], step: [100/118], loss: 0.4399\n",
      "  epoch: [3/25], step: [100/116], loss: 0.9608\n",
      "  epoch: [3/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [3/25], step: [200/231], loss: 0.2266\n",
      "  epoch: [3/25], step: [100/118], loss: 0.4404\n",
      "  epoch: [4/25], step: [100/116], loss: 0.9605\n",
      "  epoch: [4/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [4/25], step: [200/231], loss: 0.2273\n",
      "  epoch: [4/25], step: [100/118], loss: 0.4407\n",
      "  epoch: [5/25], step: [100/116], loss: 0.9604\n",
      "  epoch: [5/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [5/25], step: [200/231], loss: 0.2276\n",
      "  epoch: [5/25], step: [100/118], loss: 0.4408\n",
      "  epoch: [6/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [6/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [6/25], step: [200/231], loss: 0.2278\n",
      "  epoch: [6/25], step: [100/118], loss: 0.4408\n",
      "  epoch: [7/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [7/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [7/25], step: [200/231], loss: 0.2278\n",
      "  epoch: [7/25], step: [100/118], loss: 0.4409\n",
      "  epoch: [8/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [8/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [8/25], step: [200/231], loss: 0.2279\n",
      "  epoch: [8/25], step: [100/118], loss: 0.4409\n",
      "  epoch: [9/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [9/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [9/25], step: [200/231], loss: 0.2279\n",
      "  epoch: [9/25], step: [100/118], loss: 0.4409\n",
      "  epoch: [10/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [10/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [10/25], step: [200/231], loss: 0.2279\n",
      "  epoch: [10/25], step: [100/118], loss: 0.4409\n",
      "  epoch: [11/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [11/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [11/25], step: [200/231], loss: 0.2279\n",
      "  epoch: [11/25], step: [100/118], loss: 0.4409\n",
      "  epoch: [12/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [12/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [12/25], step: [200/231], loss: 0.2279\n",
      "  epoch: [12/25], step: [100/118], loss: 0.4409\n",
      "  epoch: [13/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [13/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [13/25], step: [200/231], loss: 0.2279\n",
      "  epoch: [13/25], step: [100/118], loss: 0.4409\n",
      "  epoch: [14/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [14/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [14/25], step: [200/231], loss: 0.2279\n",
      "  epoch: [14/25], step: [100/118], loss: 0.4409\n",
      "  epoch: [15/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [15/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [15/25], step: [200/231], loss: 0.2279\n",
      "  epoch: [15/25], step: [100/118], loss: 0.4409\n",
      "  epoch: [16/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [16/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [16/25], step: [200/231], loss: 0.2279\n",
      "  epoch: [16/25], step: [100/118], loss: 0.4409\n",
      "  epoch: [17/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [17/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [17/25], step: [200/231], loss: 0.2279\n",
      "  epoch: [17/25], step: [100/118], loss: 0.4409\n",
      "  epoch: [18/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [18/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [18/25], step: [200/231], loss: 0.2279\n",
      "  epoch: [18/25], step: [100/118], loss: 0.4409\n",
      "  epoch: [19/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [19/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [19/25], step: [200/231], loss: 0.2279\n",
      "  epoch: [19/25], step: [100/118], loss: 0.4409\n",
      "  epoch: [20/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [20/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [20/25], step: [200/231], loss: 0.2279\n",
      "  epoch: [20/25], step: [100/118], loss: 0.4409\n",
      "  epoch: [21/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [21/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [21/25], step: [200/231], loss: 0.2279\n",
      "  epoch: [21/25], step: [100/118], loss: 0.4409\n",
      "  epoch: [22/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [22/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [22/25], step: [200/231], loss: 0.2279\n",
      "  epoch: [22/25], step: [100/118], loss: 0.4409\n",
      "  epoch: [23/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [23/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [23/25], step: [200/231], loss: 0.2279\n",
      "  epoch: [23/25], step: [100/118], loss: 0.4409\n",
      "  epoch: [24/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [24/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [24/25], step: [200/231], loss: 0.2279\n",
      "  epoch: [24/25], step: [100/118], loss: 0.4409\n",
      "  epoch: [25/25], step: [100/116], loss: 0.9603\n",
      "  epoch: [25/25], step: [100/231], loss: 1.3309\n",
      "  epoch: [25/25], step: [200/231], loss: 0.2279\n",
      "  epoch: [25/25], step: [100/118], loss: 0.4409\n",
      "learning rate 0.0100\n",
      "  epoch: [1/30], step: [100/116], loss: 0.9424\n",
      "  epoch: [1/30], step: [100/231], loss: 0.9020\n",
      "  epoch: [1/30], step: [200/231], loss: 0.2055\n",
      "  epoch: [1/30], step: [100/118], loss: 0.4298\n",
      "  epoch: [2/30], step: [100/116], loss: 0.9424\n",
      "  epoch: [2/30], step: [100/231], loss: 0.9022\n",
      "  epoch: [2/30], step: [200/231], loss: 0.2064\n",
      "  epoch: [2/30], step: [100/118], loss: 0.4290\n",
      "  epoch: [3/30], step: [100/116], loss: 0.9421\n",
      "  epoch: [3/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [3/30], step: [200/231], loss: 0.2065\n",
      "  epoch: [3/30], step: [100/118], loss: 0.4287\n",
      "  epoch: [4/30], step: [100/116], loss: 0.9421\n",
      "  epoch: [4/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [4/30], step: [200/231], loss: 0.2065\n",
      "  epoch: [4/30], step: [100/118], loss: 0.4286\n",
      "  epoch: [5/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [5/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [5/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [5/30], step: [100/118], loss: 0.4286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch: [6/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [6/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [6/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [6/30], step: [100/118], loss: 0.4286\n",
      "  epoch: [7/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [7/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [7/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [7/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [8/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [8/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [8/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [8/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [9/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [9/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [9/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [9/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [10/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [10/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [10/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [10/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [11/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [11/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [11/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [11/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [12/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [12/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [12/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [12/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [13/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [13/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [13/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [13/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [14/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [14/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [14/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [14/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [15/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [15/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [15/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [15/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [16/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [16/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [16/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [16/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [17/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [17/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [17/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [17/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [18/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [18/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [18/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [18/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [19/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [19/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [19/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [19/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [20/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [20/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [20/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [20/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [21/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [21/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [21/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [21/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [22/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [22/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [22/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [22/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [23/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [23/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [23/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [23/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [24/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [24/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [24/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [24/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [25/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [25/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [25/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [25/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [26/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [26/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [26/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [26/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [27/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [27/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [27/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [27/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [28/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [28/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [28/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [28/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [29/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [29/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [29/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [29/30], step: [100/118], loss: 0.4285\n",
      "  epoch: [30/30], step: [100/116], loss: 0.9420\n",
      "  epoch: [30/30], step: [100/231], loss: 0.9023\n",
      "  epoch: [30/30], step: [200/231], loss: 0.2066\n",
      "  epoch: [30/30], step: [100/118], loss: 0.4285\n",
      "learning rate 0.0050\n",
      "  epoch: [1/40], step: [100/116], loss: 0.9419\n",
      "  epoch: [1/40], step: [100/231], loss: 0.8865\n",
      "  epoch: [1/40], step: [200/231], loss: 0.2194\n",
      "  epoch: [1/40], step: [100/118], loss: 0.4761\n",
      "  epoch: [2/40], step: [100/116], loss: 0.9494\n",
      "  epoch: [2/40], step: [100/231], loss: 0.8841\n",
      "  epoch: [2/40], step: [200/231], loss: 0.2184\n",
      "  epoch: [2/40], step: [100/118], loss: 0.4741\n",
      "  epoch: [3/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [3/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [3/40], step: [200/231], loss: 0.2186\n",
      "  epoch: [3/40], step: [100/118], loss: 0.4739\n",
      "  epoch: [4/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [4/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [4/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [4/40], step: [100/118], loss: 0.4739\n",
      "  epoch: [5/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [5/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [5/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [5/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [6/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [6/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [6/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [6/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [7/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [7/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [7/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [7/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [8/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [8/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [8/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [8/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [9/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [9/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [9/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [9/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [10/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [10/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [10/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [10/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [11/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [11/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [11/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [11/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [12/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [12/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [12/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [12/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [13/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [13/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [13/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [13/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [14/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [14/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [14/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [14/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [15/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [15/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [15/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [15/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [16/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [16/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [16/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [16/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [17/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [17/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [17/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [17/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [18/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [18/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [18/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [18/40], step: [100/118], loss: 0.4738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch: [19/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [19/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [19/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [19/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [20/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [20/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [20/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [20/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [21/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [21/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [21/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [21/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [22/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [22/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [22/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [22/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [23/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [23/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [23/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [23/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [24/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [24/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [24/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [24/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [25/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [25/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [25/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [25/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [26/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [26/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [26/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [26/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [27/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [27/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [27/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [27/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [28/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [28/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [28/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [28/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [29/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [29/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [29/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [29/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [30/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [30/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [30/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [30/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [31/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [31/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [31/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [31/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [32/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [32/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [32/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [32/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [33/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [33/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [33/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [33/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [34/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [34/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [34/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [34/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [35/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [35/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [35/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [35/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [36/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [36/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [36/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [36/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [37/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [37/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [37/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [37/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [38/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [38/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [38/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [38/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [39/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [39/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [39/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [39/40], step: [100/118], loss: 0.4738\n",
      "  epoch: [40/40], step: [100/116], loss: 0.9496\n",
      "  epoch: [40/40], step: [100/231], loss: 0.8840\n",
      "  epoch: [40/40], step: [200/231], loss: 0.2187\n",
      "  epoch: [40/40], step: [100/118], loss: 0.4738\n",
      "learning rate 0.0010\n",
      "  epoch: [1/50], step: [100/116], loss: 0.9557\n",
      "  epoch: [1/50], step: [100/231], loss: 0.8692\n",
      "  epoch: [1/50], step: [200/231], loss: 0.2403\n",
      "  epoch: [1/50], step: [100/118], loss: 0.5646\n",
      "  epoch: [2/50], step: [100/116], loss: 0.9644\n",
      "  epoch: [2/50], step: [100/231], loss: 0.8673\n",
      "  epoch: [2/50], step: [200/231], loss: 0.2337\n",
      "  epoch: [2/50], step: [100/118], loss: 0.5556\n",
      "  epoch: [3/50], step: [100/116], loss: 0.9684\n",
      "  epoch: [3/50], step: [100/231], loss: 0.8661\n",
      "  epoch: [3/50], step: [200/231], loss: 0.2303\n",
      "  epoch: [3/50], step: [100/118], loss: 0.5510\n",
      "  epoch: [4/50], step: [100/116], loss: 0.9705\n",
      "  epoch: [4/50], step: [100/231], loss: 0.8654\n",
      "  epoch: [4/50], step: [200/231], loss: 0.2285\n",
      "  epoch: [4/50], step: [100/118], loss: 0.5485\n",
      "  epoch: [5/50], step: [100/116], loss: 0.9716\n",
      "  epoch: [5/50], step: [100/231], loss: 0.8650\n",
      "  epoch: [5/50], step: [200/231], loss: 0.2276\n",
      "  epoch: [5/50], step: [100/118], loss: 0.5473\n",
      "  epoch: [6/50], step: [100/116], loss: 0.9721\n",
      "  epoch: [6/50], step: [100/231], loss: 0.8648\n",
      "  epoch: [6/50], step: [200/231], loss: 0.2271\n",
      "  epoch: [6/50], step: [100/118], loss: 0.5466\n",
      "  epoch: [7/50], step: [100/116], loss: 0.9724\n",
      "  epoch: [7/50], step: [100/231], loss: 0.8647\n",
      "  epoch: [7/50], step: [200/231], loss: 0.2269\n",
      "  epoch: [7/50], step: [100/118], loss: 0.5462\n",
      "  epoch: [8/50], step: [100/116], loss: 0.9726\n",
      "  epoch: [8/50], step: [100/231], loss: 0.8646\n",
      "  epoch: [8/50], step: [200/231], loss: 0.2268\n",
      "  epoch: [8/50], step: [100/118], loss: 0.5460\n",
      "  epoch: [9/50], step: [100/116], loss: 0.9727\n",
      "  epoch: [9/50], step: [100/231], loss: 0.8646\n",
      "  epoch: [9/50], step: [200/231], loss: 0.2267\n",
      "  epoch: [9/50], step: [100/118], loss: 0.5460\n",
      "  epoch: [10/50], step: [100/116], loss: 0.9727\n",
      "  epoch: [10/50], step: [100/231], loss: 0.8646\n",
      "  epoch: [10/50], step: [200/231], loss: 0.2267\n",
      "  epoch: [10/50], step: [100/118], loss: 0.5459\n",
      "  epoch: [11/50], step: [100/116], loss: 0.9727\n",
      "  epoch: [11/50], step: [100/231], loss: 0.8646\n",
      "  epoch: [11/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [11/50], step: [100/118], loss: 0.5459\n",
      "  epoch: [12/50], step: [100/116], loss: 0.9727\n",
      "  epoch: [12/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [12/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [12/50], step: [100/118], loss: 0.5459\n",
      "  epoch: [13/50], step: [100/116], loss: 0.9727\n",
      "  epoch: [13/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [13/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [13/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [14/50], step: [100/116], loss: 0.9727\n",
      "  epoch: [14/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [14/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [14/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [15/50], step: [100/116], loss: 0.9727\n",
      "  epoch: [15/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [15/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [15/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [16/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [16/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [16/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [16/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [17/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [17/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [17/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [17/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [18/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [18/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [18/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [18/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [19/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [19/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [19/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [19/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [20/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [20/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [20/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [20/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [21/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [21/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [21/50], step: [200/231], loss: 0.2266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch: [21/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [22/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [22/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [22/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [22/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [23/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [23/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [23/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [23/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [24/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [24/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [24/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [24/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [25/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [25/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [25/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [25/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [26/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [26/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [26/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [26/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [27/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [27/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [27/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [27/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [28/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [28/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [28/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [28/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [29/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [29/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [29/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [29/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [30/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [30/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [30/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [30/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [31/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [31/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [31/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [31/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [32/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [32/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [32/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [32/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [33/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [33/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [33/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [33/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [34/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [34/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [34/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [34/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [35/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [35/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [35/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [35/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [36/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [36/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [36/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [36/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [37/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [37/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [37/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [37/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [38/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [38/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [38/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [38/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [39/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [39/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [39/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [39/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [40/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [40/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [40/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [40/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [41/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [41/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [41/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [41/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [42/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [42/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [42/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [42/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [43/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [43/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [43/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [43/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [44/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [44/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [44/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [44/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [45/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [45/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [45/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [45/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [46/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [46/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [46/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [46/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [47/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [47/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [47/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [47/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [48/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [48/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [48/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [48/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [49/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [49/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [49/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [49/50], step: [100/118], loss: 0.5458\n",
      "  epoch: [50/50], step: [100/116], loss: 0.9728\n",
      "  epoch: [50/50], step: [100/231], loss: 0.8645\n",
      "  epoch: [50/50], step: [200/231], loss: 0.2266\n",
      "  epoch: [50/50], step: [100/118], loss: 0.5458\n",
      "learning rate 0.0005\n",
      "  epoch: [1/50], step: [100/116], loss: 0.9764\n",
      "  epoch: [1/50], step: [100/231], loss: 0.8620\n",
      "  epoch: [1/50], step: [200/231], loss: 0.2262\n",
      "  epoch: [1/50], step: [100/118], loss: 0.5558\n",
      "  epoch: [2/50], step: [100/116], loss: 0.9767\n",
      "  epoch: [2/50], step: [100/231], loss: 0.8622\n",
      "  epoch: [2/50], step: [200/231], loss: 0.2263\n",
      "  epoch: [2/50], step: [100/118], loss: 0.5560\n",
      "  epoch: [3/50], step: [100/116], loss: 0.9767\n",
      "  epoch: [3/50], step: [100/231], loss: 0.8623\n",
      "  epoch: [3/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [3/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [4/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [4/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [4/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [4/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [5/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [5/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [5/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [5/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [6/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [6/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [6/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [6/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [7/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [7/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [7/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [7/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [8/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [8/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [8/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [8/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [9/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [9/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [9/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [9/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [10/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [10/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [10/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [10/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [11/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [11/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [11/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [11/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [12/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [12/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [12/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [12/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [13/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [13/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [13/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [13/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [14/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [14/50], step: [100/231], loss: 0.8624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch: [14/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [14/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [15/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [15/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [15/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [15/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [16/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [16/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [16/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [16/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [17/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [17/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [17/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [17/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [18/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [18/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [18/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [18/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [19/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [19/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [19/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [19/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [20/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [20/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [20/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [20/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [21/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [21/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [21/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [21/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [22/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [22/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [22/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [22/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [23/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [23/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [23/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [23/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [24/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [24/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [24/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [24/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [25/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [25/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [25/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [25/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [26/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [26/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [26/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [26/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [27/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [27/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [27/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [27/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [28/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [28/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [28/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [28/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [29/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [29/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [29/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [29/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [30/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [30/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [30/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [30/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [31/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [31/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [31/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [31/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [32/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [32/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [32/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [32/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [33/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [33/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [33/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [33/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [34/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [34/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [34/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [34/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [35/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [35/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [35/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [35/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [36/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [36/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [36/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [36/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [37/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [37/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [37/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [37/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [38/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [38/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [38/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [38/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [39/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [39/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [39/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [39/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [40/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [40/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [40/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [40/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [41/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [41/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [41/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [41/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [42/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [42/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [42/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [42/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [43/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [43/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [43/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [43/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [44/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [44/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [44/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [44/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [45/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [45/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [45/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [45/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [46/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [46/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [46/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [46/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [47/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [47/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [47/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [47/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [48/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [48/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [48/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [48/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [49/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [49/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [49/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [49/50], step: [100/118], loss: 0.5561\n",
      "  epoch: [50/50], step: [100/116], loss: 0.9768\n",
      "  epoch: [50/50], step: [100/231], loss: 0.8624\n",
      "  epoch: [50/50], step: [200/231], loss: 0.2264\n",
      "  epoch: [50/50], step: [100/118], loss: 0.5561\n",
      "learning rate 0.0001\n",
      "  epoch: [1/50], step: [100/116], loss: 0.9795\n",
      "  epoch: [1/50], step: [100/231], loss: 0.8603\n",
      "  epoch: [1/50], step: [200/231], loss: 0.2261\n",
      "  epoch: [1/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [2/50], step: [100/116], loss: 0.9796\n",
      "  epoch: [2/50], step: [100/231], loss: 0.8604\n",
      "  epoch: [2/50], step: [200/231], loss: 0.2261\n",
      "  epoch: [2/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [3/50], step: [100/116], loss: 0.9796\n",
      "  epoch: [3/50], step: [100/231], loss: 0.8605\n",
      "  epoch: [3/50], step: [200/231], loss: 0.2261\n",
      "  epoch: [3/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [4/50], step: [100/116], loss: 0.9797\n",
      "  epoch: [4/50], step: [100/231], loss: 0.8605\n",
      "  epoch: [4/50], step: [200/231], loss: 0.2261\n",
      "  epoch: [4/50], step: [100/118], loss: 0.5643\n",
      "  epoch: [5/50], step: [100/116], loss: 0.9797\n",
      "  epoch: [5/50], step: [100/231], loss: 0.8606\n",
      "  epoch: [5/50], step: [200/231], loss: 0.2261\n",
      "  epoch: [5/50], step: [100/118], loss: 0.5643\n",
      "  epoch: [6/50], step: [100/116], loss: 0.9798\n",
      "  epoch: [6/50], step: [100/231], loss: 0.8606\n",
      "  epoch: [6/50], step: [200/231], loss: 0.2260\n",
      "  epoch: [6/50], step: [100/118], loss: 0.5643\n",
      "  epoch: [7/50], step: [100/116], loss: 0.9798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch: [7/50], step: [100/231], loss: 0.8606\n",
      "  epoch: [7/50], step: [200/231], loss: 0.2260\n",
      "  epoch: [7/50], step: [100/118], loss: 0.5643\n",
      "  epoch: [8/50], step: [100/116], loss: 0.9798\n",
      "  epoch: [8/50], step: [100/231], loss: 0.8607\n",
      "  epoch: [8/50], step: [200/231], loss: 0.2260\n",
      "  epoch: [8/50], step: [100/118], loss: 0.5643\n",
      "  epoch: [9/50], step: [100/116], loss: 0.9799\n",
      "  epoch: [9/50], step: [100/231], loss: 0.8607\n",
      "  epoch: [9/50], step: [200/231], loss: 0.2260\n",
      "  epoch: [9/50], step: [100/118], loss: 0.5643\n",
      "  epoch: [10/50], step: [100/116], loss: 0.9799\n",
      "  epoch: [10/50], step: [100/231], loss: 0.8607\n",
      "  epoch: [10/50], step: [200/231], loss: 0.2260\n",
      "  epoch: [10/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [11/50], step: [100/116], loss: 0.9799\n",
      "  epoch: [11/50], step: [100/231], loss: 0.8607\n",
      "  epoch: [11/50], step: [200/231], loss: 0.2260\n",
      "  epoch: [11/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [12/50], step: [100/116], loss: 0.9799\n",
      "  epoch: [12/50], step: [100/231], loss: 0.8607\n",
      "  epoch: [12/50], step: [200/231], loss: 0.2260\n",
      "  epoch: [12/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [13/50], step: [100/116], loss: 0.9800\n",
      "  epoch: [13/50], step: [100/231], loss: 0.8608\n",
      "  epoch: [13/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [13/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [14/50], step: [100/116], loss: 0.9800\n",
      "  epoch: [14/50], step: [100/231], loss: 0.8608\n",
      "  epoch: [14/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [14/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [15/50], step: [100/116], loss: 0.9800\n",
      "  epoch: [15/50], step: [100/231], loss: 0.8608\n",
      "  epoch: [15/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [15/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [16/50], step: [100/116], loss: 0.9800\n",
      "  epoch: [16/50], step: [100/231], loss: 0.8608\n",
      "  epoch: [16/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [16/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [17/50], step: [100/116], loss: 0.9800\n",
      "  epoch: [17/50], step: [100/231], loss: 0.8608\n",
      "  epoch: [17/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [17/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [18/50], step: [100/116], loss: 0.9800\n",
      "  epoch: [18/50], step: [100/231], loss: 0.8608\n",
      "  epoch: [18/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [18/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [19/50], step: [100/116], loss: 0.9800\n",
      "  epoch: [19/50], step: [100/231], loss: 0.8608\n",
      "  epoch: [19/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [19/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [20/50], step: [100/116], loss: 0.9800\n",
      "  epoch: [20/50], step: [100/231], loss: 0.8608\n",
      "  epoch: [20/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [20/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [21/50], step: [100/116], loss: 0.9800\n",
      "  epoch: [21/50], step: [100/231], loss: 0.8608\n",
      "  epoch: [21/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [21/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [22/50], step: [100/116], loss: 0.9800\n",
      "  epoch: [22/50], step: [100/231], loss: 0.8608\n",
      "  epoch: [22/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [22/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [23/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [23/50], step: [100/231], loss: 0.8608\n",
      "  epoch: [23/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [23/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [24/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [24/50], step: [100/231], loss: 0.8608\n",
      "  epoch: [24/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [24/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [25/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [25/50], step: [100/231], loss: 0.8608\n",
      "  epoch: [25/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [25/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [26/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [26/50], step: [100/231], loss: 0.8608\n",
      "  epoch: [26/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [26/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [27/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [27/50], step: [100/231], loss: 0.8608\n",
      "  epoch: [27/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [27/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [28/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [28/50], step: [100/231], loss: 0.8608\n",
      "  epoch: [28/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [28/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [29/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [29/50], step: [100/231], loss: 0.8608\n",
      "  epoch: [29/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [29/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [30/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [30/50], step: [100/231], loss: 0.8608\n",
      "  epoch: [30/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [30/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [31/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [31/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [31/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [31/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [32/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [32/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [32/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [32/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [33/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [33/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [33/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [33/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [34/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [34/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [34/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [34/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [35/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [35/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [35/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [35/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [36/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [36/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [36/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [36/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [37/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [37/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [37/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [37/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [38/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [38/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [38/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [38/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [39/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [39/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [39/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [39/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [40/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [40/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [40/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [40/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [41/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [41/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [41/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [41/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [42/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [42/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [42/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [42/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [43/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [43/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [43/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [43/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [44/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [44/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [44/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [44/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [45/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [45/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [45/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [45/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [46/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [46/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [46/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [46/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [47/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [47/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [47/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [47/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [48/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [48/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [48/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [48/50], step: [100/118], loss: 0.5642\n",
      "  epoch: [49/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [49/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [49/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [49/50], step: [100/118], loss: 0.5642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch: [50/50], step: [100/116], loss: 0.9801\n",
      "  epoch: [50/50], step: [100/231], loss: 0.8609\n",
      "  epoch: [50/50], step: [200/231], loss: 0.2259\n",
      "  epoch: [50/50], step: [100/118], loss: 0.5642\n"
     ]
    }
   ],
   "source": [
    "# Train with changing learning rate on number of epochs\n",
    "for lr in EPOCHS_LR:\n",
    "    no_epochs = EPOCHS_LR[lr]\n",
    "    print('learning rate %.4f' %(lr))\n",
    "\n",
    "    optimizer = torch.optim.Adam(lstm_nn.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in np.arange(no_epochs):\n",
    "        for f in training_files:\n",
    "            train_loader = DataLoader(dataset=training_sets[f], batch_size=BATCH_SIZE, shuffle=False)    \n",
    "            lstm_nn.init_hidden()\n",
    "\n",
    "            for i, (X, y) in enumerate(train_loader):\n",
    "                if (len(X) != BATCH_SIZE):\n",
    "                    continue\n",
    "\n",
    "                data = Variable(X.view(-1, 1, INPUT_SIZE))\n",
    "                target = Variable(y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                prediction = lstm_nn(data)\n",
    "                loss = criterion(prediction, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if (i+1) % BATCH_SIZE == 0:\n",
    "                    print('  epoch: [%d/%d], step: [%d/%d], loss: %.4f'\n",
    "                          %(epoch+1, no_epochs, i+1, len(training_sets[f].target_tensor)//BATCH_SIZE, loss.data[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500]\n",
      "    step: [100/116], loss: 2.5588\n",
      "    step: [100/231], loss: 1.0885\n",
      "    step: [200/231], loss: 0.8383\n",
      "    step: [100/118], loss: 0.8573\n",
      "Epoch [1/500]\n",
      "    step: [100/116], loss: 2.8224\n",
      "    step: [100/231], loss: 0.9594\n",
      "    step: [200/231], loss: 0.7714\n",
      "    step: [100/118], loss: 1.2330\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-505775e418f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b0c7eb54ae4c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         )\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutogradRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, weight, hidden)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mnexth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, hidden, weight)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mview\u001b[0;34m(self, *sizes)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mView\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/miniconda3/lib/python3.6/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, sizes)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Changing learning rate\n",
    "lridx = -1\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "while epoch < NUM_EPOCHS:\n",
    "    print('Epoch [%d/%d]' %(epoch+1, NUM_EPOCHS))\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        lridx += 1\n",
    "        optimizer = torch.optim.Adam(lstm_nn.parameters(), lr=LRS[lridx])\n",
    "    \n",
    "    for f in training_files:\n",
    "#         print('  training set: %s' %(f[f.find('/')+1:]))        \n",
    "        train_loader = DataLoader(dataset=training_sets[f], batch_size=BATCH_SIZE, shuffle=False)    \n",
    "        lstm_nn.init_hidden()\n",
    "\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            if (len(X) != BATCH_SIZE):\n",
    "                continue\n",
    "\n",
    "            data = Variable(X.view(-1, 1, INPUT_SIZE))\n",
    "            target = Variable(y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction = lstm_nn(data)\n",
    "            loss = criterion(prediction, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % BATCH_SIZE == 0:\n",
    "                \n",
    "                print('    step: [%d/%d], loss: %.4f'\n",
    "                      %(i+1, len(training_sets[f].target_tensor)//BATCH_SIZE, loss.data[0]))\n",
    "\n",
    "print('Training done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n",
      "    step: [50/142], loss: 3.1737\n",
      "    step: [100/142], loss: 5.7125\n",
      "    step: [50/232], loss: 1.2703\n",
      "    step: [100/232], loss: 1.4347\n",
      "    step: [150/232], loss: 0.1856\n",
      "    step: [200/232], loss: 0.3470\n",
      "    step: [50/462], loss: 0.5526\n",
      "    step: [100/462], loss: 0.1797\n",
      "    step: [150/462], loss: 1.8688\n",
      "    step: [200/462], loss: 0.4442\n",
      "    step: [250/462], loss: 0.2204\n",
      "    step: [300/462], loss: 1.7494\n",
      "    step: [350/462], loss: 1.0708\n",
      "    step: [400/462], loss: 0.1297\n",
      "    step: [450/462], loss: 0.6466\n",
      "    step: [50/237], loss: 0.9451\n",
      "    step: [100/237], loss: 0.1667\n",
      "    step: [150/237], loss: 0.1401\n",
      "    step: [200/237], loss: 0.2873\n",
      "    step: [50/198], loss: 0.0658\n",
      "    step: [100/198], loss: 0.0562\n",
      "    step: [150/198], loss: 0.0739\n",
      "Epoch [2/20]\n",
      "    step: [50/142], loss: 0.0370\n",
      "    step: [100/142], loss: 1.7876\n",
      "    step: [50/232], loss: 0.1572\n",
      "    step: [100/232], loss: 0.3755\n",
      "    step: [150/232], loss: 0.0637\n",
      "    step: [200/232], loss: 0.2741\n",
      "    step: [50/462], loss: 0.2666\n",
      "    step: [100/462], loss: 0.2134\n",
      "    step: [150/462], loss: 1.1785\n",
      "    step: [200/462], loss: 0.6463\n",
      "    step: [250/462], loss: 0.0840\n",
      "    step: [300/462], loss: 1.1730\n",
      "    step: [350/462], loss: 0.8744\n",
      "    step: [400/462], loss: 0.3429\n",
      "    step: [450/462], loss: 0.3647\n",
      "    step: [50/237], loss: 0.9459\n",
      "    step: [100/237], loss: 0.3446\n",
      "    step: [150/237], loss: 0.2386\n",
      "    step: [200/237], loss: 0.2430\n",
      "    step: [50/198], loss: 0.0783\n",
      "    step: [100/198], loss: 0.0747\n",
      "    step: [150/198], loss: 0.0973\n",
      "Epoch [3/20]\n",
      "    step: [50/142], loss: 0.0763\n",
      "    step: [100/142], loss: 1.6516\n",
      "    step: [50/232], loss: 0.1753\n",
      "    step: [100/232], loss: 0.2337\n",
      "    step: [150/232], loss: 0.0917\n",
      "    step: [200/232], loss: 0.2788\n",
      "    step: [50/462], loss: 0.1517\n",
      "    step: [100/462], loss: 0.1288\n",
      "    step: [150/462], loss: 0.8412\n",
      "    step: [200/462], loss: 0.6273\n",
      "    step: [250/462], loss: 0.1472\n",
      "    step: [300/462], loss: 0.7931\n",
      "    step: [350/462], loss: 0.7441\n",
      "    step: [400/462], loss: 0.4405\n",
      "    step: [450/462], loss: 0.1476\n",
      "    step: [50/237], loss: 0.2121\n",
      "    step: [100/237], loss: 0.1785\n",
      "    step: [150/237], loss: 0.0662\n",
      "    step: [200/237], loss: 0.1943\n",
      "    step: [50/198], loss: 0.1217\n",
      "    step: [100/198], loss: 0.1226\n",
      "    step: [150/198], loss: 0.0838\n",
      "Epoch [4/20]\n",
      "    step: [50/142], loss: 0.1423\n",
      "    step: [100/142], loss: 1.6033\n",
      "    step: [50/232], loss: 0.2764\n",
      "    step: [100/232], loss: 0.1201\n",
      "    step: [150/232], loss: 0.0689\n",
      "    step: [200/232], loss: 0.3215\n",
      "    step: [50/462], loss: 0.1006\n",
      "    step: [100/462], loss: 0.0947\n",
      "    step: [150/462], loss: 0.6249\n",
      "    step: [200/462], loss: 0.3573\n",
      "    step: [250/462], loss: 0.0549\n",
      "    step: [300/462], loss: 0.5213\n",
      "    step: [350/462], loss: 0.7109\n",
      "    step: [400/462], loss: 0.2710\n",
      "    step: [450/462], loss: 0.0740\n",
      "    step: [50/237], loss: 0.1933\n",
      "    step: [100/237], loss: 0.1687\n",
      "    step: [150/237], loss: 0.0962\n",
      "    step: [200/237], loss: 0.2150\n",
      "    step: [50/198], loss: 0.1135\n",
      "    step: [100/198], loss: 0.0663\n",
      "    step: [150/198], loss: 0.1372\n",
      "Epoch [5/20]\n",
      "    step: [50/142], loss: 0.0369\n",
      "    step: [100/142], loss: 1.5855\n",
      "    step: [50/232], loss: 0.2282\n",
      "    step: [100/232], loss: 0.0888\n",
      "    step: [150/232], loss: 0.0762\n",
      "    step: [200/232], loss: 0.2930\n",
      "    step: [50/462], loss: 0.0806\n",
      "    step: [100/462], loss: 0.0862\n",
      "    step: [150/462], loss: 0.4534\n",
      "    step: [200/462], loss: 0.4302\n",
      "    step: [250/462], loss: 0.1032\n",
      "    step: [300/462], loss: 0.4057\n",
      "    step: [350/462], loss: 0.7382\n",
      "    step: [400/462], loss: 0.2630\n",
      "    step: [450/462], loss: 0.0593\n",
      "    step: [50/237], loss: 0.2130\n",
      "    step: [100/237], loss: 0.1770\n",
      "    step: [150/237], loss: 0.0814\n",
      "    step: [200/237], loss: 0.2050\n",
      "    step: [50/198], loss: 0.1684\n",
      "    step: [100/198], loss: 0.0649\n",
      "    step: [150/198], loss: 0.1567\n",
      "Epoch [6/20]\n",
      "    step: [50/142], loss: 0.0431\n",
      "    step: [100/142], loss: 1.5906\n",
      "    step: [50/232], loss: 0.1672\n",
      "    step: [100/232], loss: 0.0699\n",
      "    step: [150/232], loss: 0.0821\n",
      "    step: [200/232], loss: 0.3481\n",
      "    step: [50/462], loss: 0.0759\n",
      "    step: [100/462], loss: 0.0960\n",
      "    step: [150/462], loss: 0.3460\n",
      "    step: [200/462], loss: 0.2926\n",
      "    step: [250/462], loss: 0.0951\n",
      "    step: [300/462], loss: 0.3630\n",
      "    step: [350/462], loss: 0.7657\n",
      "    step: [400/462], loss: 0.1285\n",
      "    step: [450/462], loss: 0.0508\n",
      "    step: [50/237], loss: 0.2151\n",
      "    step: [100/237], loss: 0.1623\n",
      "    step: [150/237], loss: 0.0657\n",
      "    step: [200/237], loss: 0.1872\n",
      "    step: [50/198], loss: 0.1950\n",
      "    step: [100/198], loss: 0.0462\n",
      "    step: [150/198], loss: 0.1896\n",
      "Epoch [7/20]\n",
      "    step: [50/142], loss: 0.0443\n",
      "    step: [100/142], loss: 1.6046\n",
      "    step: [50/232], loss: 0.1696\n",
      "    step: [100/232], loss: 0.0510\n",
      "    step: [150/232], loss: 0.0900\n",
      "    step: [200/232], loss: 0.3688\n",
      "    step: [50/462], loss: 0.0740\n",
      "    step: [100/462], loss: 0.0907\n",
      "    step: [150/462], loss: 0.2894\n",
      "    step: [200/462], loss: 0.3348\n",
      "    step: [250/462], loss: 0.0907\n",
      "    step: [300/462], loss: 0.2689\n",
      "    step: [350/462], loss: 0.7519\n",
      "    step: [400/462], loss: 0.1119\n",
      "    step: [450/462], loss: 0.0273\n",
      "    step: [50/237], loss: 0.1534\n",
      "    step: [100/237], loss: 0.1470\n",
      "    step: [150/237], loss: 0.0635\n",
      "    step: [200/237], loss: 0.1767\n",
      "    step: [50/198], loss: 0.2519\n",
      "    step: [100/198], loss: 0.0391\n",
      "    step: [150/198], loss: 0.1896\n",
      "Epoch [8/20]\n",
      "    step: [50/142], loss: 0.0509\n",
      "    step: [100/142], loss: 1.5392\n",
      "    step: [50/232], loss: 0.2085\n",
      "    step: [100/232], loss: 0.0422\n",
      "    step: [150/232], loss: 0.1028\n",
      "    step: [200/232], loss: 0.3486\n",
      "    step: [50/462], loss: 0.0723\n",
      "    step: [100/462], loss: 0.0852\n",
      "    step: [150/462], loss: 0.2660\n",
      "    step: [200/462], loss: 0.3498\n",
      "    step: [250/462], loss: 0.0790\n",
      "    step: [300/462], loss: 0.2426\n",
      "    step: [350/462], loss: 0.7004\n",
      "    step: [400/462], loss: 0.0831\n",
      "    step: [450/462], loss: 0.0194\n",
      "    step: [50/237], loss: 0.1941\n",
      "    step: [100/237], loss: 0.1352\n",
      "    step: [150/237], loss: 0.0496\n",
      "    step: [200/237], loss: 0.1828\n",
      "    step: [50/198], loss: 0.2636\n",
      "    step: [100/198], loss: 0.0265\n",
      "    step: [150/198], loss: 0.0923\n",
      "Epoch [9/20]\n",
      "    step: [50/142], loss: 0.0510\n",
      "    step: [100/142], loss: 1.5708\n",
      "    step: [50/232], loss: 0.1718\n",
      "    step: [100/232], loss: 0.1847\n",
      "    step: [150/232], loss: 0.1272\n",
      "    step: [200/232], loss: 0.2816\n",
      "    step: [50/462], loss: 0.0787\n",
      "    step: [100/462], loss: 0.0789\n",
      "    step: [150/462], loss: 0.2472\n",
      "    step: [200/462], loss: 0.4431\n",
      "    step: [250/462], loss: 0.1062\n",
      "    step: [300/462], loss: 0.1990\n",
      "    step: [350/462], loss: 0.7086\n",
      "    step: [400/462], loss: 0.1213\n",
      "    step: [450/462], loss: 0.0096\n",
      "    step: [50/237], loss: 0.1606\n",
      "    step: [100/237], loss: 0.1278\n",
      "    step: [150/237], loss: 0.0559\n",
      "    step: [200/237], loss: 0.1737\n",
      "    step: [50/198], loss: 0.2510\n",
      "    step: [100/198], loss: 0.0234\n",
      "    step: [150/198], loss: 0.1183\n",
      "Epoch [10/20]\n",
      "    step: [50/142], loss: 0.0582\n",
      "    step: [100/142], loss: 1.5659\n",
      "    step: [50/232], loss: 0.1829\n",
      "    step: [100/232], loss: 0.1511\n",
      "    step: [150/232], loss: 0.1396\n",
      "    step: [200/232], loss: 0.2744\n",
      "    step: [50/462], loss: 0.0812\n",
      "    step: [100/462], loss: 0.0795\n",
      "    step: [150/462], loss: 0.2175\n",
      "    step: [200/462], loss: 0.4003\n",
      "    step: [250/462], loss: 0.1027\n",
      "    step: [300/462], loss: 0.1860\n",
      "    step: [350/462], loss: 0.7966\n",
      "    step: [400/462], loss: 0.0690\n",
      "    step: [450/462], loss: 0.0116\n",
      "    step: [50/237], loss: 0.1496\n",
      "    step: [100/237], loss: 0.1312\n",
      "    step: [150/237], loss: 0.0683\n",
      "    step: [200/237], loss: 0.1656\n",
      "    step: [50/198], loss: 0.2679\n",
      "    step: [100/198], loss: 0.0226\n",
      "    step: [150/198], loss: 0.1051\n",
      "Epoch [11/20]\n",
      "    step: [50/142], loss: 0.0660\n",
      "    step: [100/142], loss: 1.5166\n",
      "    step: [50/232], loss: 0.1930\n",
      "    step: [100/232], loss: 0.1453\n",
      "    step: [150/232], loss: 0.1311\n",
      "    step: [200/232], loss: 0.2839\n",
      "    step: [50/462], loss: 0.0794\n",
      "    step: [100/462], loss: 0.0785\n",
      "    step: [150/462], loss: 0.2151\n",
      "    step: [200/462], loss: 0.3905\n",
      "    step: [250/462], loss: 0.0771\n",
      "    step: [300/462], loss: 0.2248\n",
      "    step: [350/462], loss: 0.5803\n",
      "    step: [400/462], loss: 0.1205\n",
      "    step: [450/462], loss: 0.0132\n",
      "    step: [50/237], loss: 0.1612\n",
      "    step: [100/237], loss: 0.1448\n",
      "    step: [150/237], loss: 0.0574\n",
      "    step: [200/237], loss: 0.1712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    step: [50/198], loss: 0.3393\n",
      "    step: [100/198], loss: 0.0169\n",
      "    step: [150/198], loss: 0.1122\n",
      "Epoch [12/20]\n",
      "    step: [50/142], loss: 0.0499\n",
      "    step: [100/142], loss: 1.4946\n",
      "    step: [50/232], loss: 0.2011\n",
      "    step: [100/232], loss: 0.0145\n",
      "    step: [150/232], loss: 0.1198\n",
      "    step: [200/232], loss: 0.3255\n",
      "    step: [50/462], loss: 0.0890\n",
      "    step: [100/462], loss: 0.0759\n",
      "    step: [150/462], loss: 0.2108\n",
      "    step: [200/462], loss: 0.4438\n",
      "    step: [250/462], loss: 0.0279\n",
      "    step: [300/462], loss: 0.2133\n",
      "    step: [350/462], loss: 0.6448\n",
      "    step: [400/462], loss: 0.1940\n",
      "    step: [450/462], loss: 0.0098\n",
      "    step: [50/237], loss: 0.1174\n",
      "    step: [100/237], loss: 0.1301\n",
      "    step: [150/237], loss: 0.1149\n",
      "    step: [200/237], loss: 0.1779\n",
      "    step: [50/198], loss: 0.3384\n",
      "    step: [100/198], loss: 0.0411\n",
      "    step: [150/198], loss: 0.0902\n",
      "Epoch [13/20]\n",
      "    step: [50/142], loss: 0.0515\n",
      "    step: [100/142], loss: 1.4561\n",
      "    step: [50/232], loss: 0.2300\n",
      "    step: [100/232], loss: 0.0086\n",
      "    step: [150/232], loss: 0.0745\n",
      "    step: [200/232], loss: 0.2504\n",
      "    step: [50/462], loss: 0.0797\n",
      "    step: [100/462], loss: 0.0502\n",
      "    step: [150/462], loss: 0.2046\n",
      "    step: [200/462], loss: 0.3876\n",
      "    step: [250/462], loss: 0.0131\n",
      "    step: [300/462], loss: 0.2257\n",
      "    step: [350/462], loss: 0.4890\n",
      "    step: [400/462], loss: 0.1126\n",
      "    step: [450/462], loss: 0.0104\n",
      "    step: [50/237], loss: 0.1050\n",
      "    step: [100/237], loss: 0.1363\n",
      "    step: [150/237], loss: 0.1066\n",
      "    step: [200/237], loss: 0.1623\n",
      "    step: [50/198], loss: 0.2353\n",
      "    step: [100/198], loss: 0.0195\n",
      "    step: [150/198], loss: 0.0957\n",
      "Epoch [14/20]\n",
      "    step: [50/142], loss: 0.0650\n",
      "    step: [100/142], loss: 1.3865\n",
      "    step: [50/232], loss: 0.1821\n",
      "    step: [100/232], loss: 0.0113\n",
      "    step: [150/232], loss: 0.0543\n",
      "    step: [200/232], loss: 0.2996\n",
      "    step: [50/462], loss: 0.0714\n",
      "    step: [100/462], loss: 0.0453\n",
      "    step: [150/462], loss: 0.1996\n",
      "    step: [200/462], loss: 0.3504\n",
      "    step: [250/462], loss: 0.0132\n",
      "    step: [300/462], loss: 0.2431\n",
      "    step: [350/462], loss: 0.4626\n",
      "    step: [400/462], loss: 0.2029\n",
      "    step: [450/462], loss: 0.0113\n",
      "    step: [50/237], loss: 0.0914\n",
      "    step: [100/237], loss: 0.1221\n",
      "    step: [150/237], loss: 0.1413\n",
      "    step: [200/237], loss: 0.1646\n",
      "    step: [50/198], loss: 0.2157\n",
      "    step: [100/198], loss: 0.0197\n",
      "    step: [150/198], loss: 0.1487\n",
      "Epoch [15/20]\n",
      "    step: [50/142], loss: 0.0697\n",
      "    step: [100/142], loss: 1.3351\n",
      "    step: [50/232], loss: 0.3466\n",
      "    step: [100/232], loss: 0.0218\n",
      "    step: [150/232], loss: 0.0617\n",
      "    step: [200/232], loss: 0.3182\n",
      "    step: [50/462], loss: 0.0823\n",
      "    step: [100/462], loss: 0.0458\n",
      "    step: [150/462], loss: 0.2211\n",
      "    step: [200/462], loss: 0.3723\n",
      "    step: [250/462], loss: 0.0145\n",
      "    step: [300/462], loss: 0.2555\n",
      "    step: [350/462], loss: 0.6043\n",
      "    step: [400/462], loss: 0.1475\n",
      "    step: [450/462], loss: 0.0127\n",
      "    step: [50/237], loss: 0.0899\n",
      "    step: [100/237], loss: 0.1315\n",
      "    step: [150/237], loss: 0.1360\n",
      "    step: [200/237], loss: 0.1897\n",
      "    step: [50/198], loss: 0.2620\n",
      "    step: [100/198], loss: 0.0249\n",
      "    step: [150/198], loss: 0.1507\n",
      "Epoch [16/20]\n",
      "    step: [50/142], loss: 0.0714\n",
      "    step: [100/142], loss: 1.3053\n",
      "    step: [50/232], loss: 0.2752\n",
      "    step: [100/232], loss: 0.0097\n",
      "    step: [150/232], loss: 0.0595\n",
      "    step: [200/232], loss: 0.2672\n",
      "    step: [50/462], loss: 0.0524\n",
      "    step: [100/462], loss: 0.0506\n",
      "    step: [150/462], loss: 0.1982\n",
      "    step: [200/462], loss: 0.3451\n",
      "    step: [250/462], loss: 0.0108\n",
      "    step: [300/462], loss: 0.2281\n",
      "    step: [350/462], loss: 0.4251\n",
      "    step: [400/462], loss: 0.1174\n",
      "    step: [450/462], loss: 0.0088\n",
      "    step: [50/237], loss: 0.0932\n",
      "    step: [100/237], loss: 0.1128\n",
      "    step: [150/237], loss: 0.0816\n",
      "    step: [200/237], loss: 0.1786\n",
      "    step: [50/198], loss: 0.1827\n",
      "    step: [100/198], loss: 0.0322\n",
      "    step: [150/198], loss: 0.0764\n",
      "Epoch [17/20]\n",
      "    step: [50/142], loss: 0.0869\n",
      "    step: [100/142], loss: 1.2137\n",
      "    step: [50/232], loss: 0.3514\n",
      "    step: [100/232], loss: 0.0120\n",
      "    step: [150/232], loss: 0.0495\n",
      "    step: [200/232], loss: 0.2449\n",
      "    step: [50/462], loss: 0.0497\n",
      "    step: [100/462], loss: 0.0515\n",
      "    step: [150/462], loss: 0.1935\n",
      "    step: [200/462], loss: 0.3110\n",
      "    step: [250/462], loss: 0.0120\n",
      "    step: [300/462], loss: 0.2118\n",
      "    step: [350/462], loss: 0.3998\n",
      "    step: [400/462], loss: 0.2179\n",
      "    step: [450/462], loss: 0.0118\n",
      "    step: [50/237], loss: 0.0943\n",
      "    step: [100/237], loss: 0.1644\n",
      "    step: [150/237], loss: 0.1671\n",
      "    step: [200/237], loss: 0.1996\n",
      "    step: [50/198], loss: 0.1538\n",
      "    step: [100/198], loss: 0.0337\n",
      "    step: [150/198], loss: 0.0899\n",
      "Epoch [18/20]\n",
      "    step: [50/142], loss: 0.1659\n",
      "    step: [100/142], loss: 1.2730\n",
      "    step: [50/232], loss: 0.2937\n",
      "    step: [100/232], loss: 0.0104\n",
      "    step: [150/232], loss: 0.0577\n",
      "    step: [200/232], loss: 0.2351\n",
      "    step: [50/462], loss: 0.0553\n",
      "    step: [100/462], loss: 0.0572\n",
      "    step: [150/462], loss: 0.1902\n",
      "    step: [200/462], loss: 0.3314\n",
      "    step: [250/462], loss: 0.0144\n",
      "    step: [300/462], loss: 0.2227\n",
      "    step: [350/462], loss: 0.5845\n",
      "    step: [400/462], loss: 0.1990\n",
      "    step: [450/462], loss: 0.0055\n",
      "    step: [50/237], loss: 0.0935\n",
      "    step: [100/237], loss: 0.1238\n",
      "    step: [150/237], loss: 0.0832\n",
      "    step: [200/237], loss: 0.1750\n",
      "    step: [50/198], loss: 0.2605\n",
      "    step: [100/198], loss: 0.0878\n",
      "    step: [150/198], loss: 0.2154\n",
      "Epoch [19/20]\n",
      "    step: [50/142], loss: 0.1315\n",
      "    step: [100/142], loss: 1.1581\n",
      "    step: [50/232], loss: 0.3638\n",
      "    step: [100/232], loss: 0.0028\n",
      "    step: [150/232], loss: 0.0576\n",
      "    step: [200/232], loss: 0.2787\n",
      "    step: [50/462], loss: 0.0337\n",
      "    step: [100/462], loss: 0.0822\n",
      "    step: [150/462], loss: 0.1872\n",
      "    step: [200/462], loss: 0.3738\n",
      "    step: [250/462], loss: 0.0202\n",
      "    step: [300/462], loss: 0.2842\n",
      "    step: [350/462], loss: 0.4901\n",
      "    step: [400/462], loss: 0.1736\n",
      "    step: [450/462], loss: 0.0139\n",
      "    step: [50/237], loss: 0.0902\n",
      "    step: [100/237], loss: 0.1321\n",
      "    step: [150/237], loss: 0.0826\n",
      "    step: [200/237], loss: 0.1814\n",
      "    step: [50/198], loss: 0.1664\n",
      "    step: [100/198], loss: 0.0209\n",
      "    step: [150/198], loss: 0.1037\n",
      "Epoch [20/20]\n",
      "    step: [50/142], loss: 0.0491\n",
      "    step: [100/142], loss: 1.1799\n",
      "    step: [50/232], loss: 0.2670\n",
      "    step: [100/232], loss: 0.0051\n",
      "    step: [150/232], loss: 0.0623\n",
      "    step: [200/232], loss: 0.2195\n",
      "    step: [50/462], loss: 0.0436\n",
      "    step: [100/462], loss: 0.0702\n",
      "    step: [150/462], loss: 0.1865\n",
      "    step: [200/462], loss: 0.3825\n",
      "    step: [250/462], loss: 0.0240\n",
      "    step: [300/462], loss: 0.2389\n",
      "    step: [350/462], loss: 0.4622\n",
      "    step: [400/462], loss: 0.1719\n",
      "    step: [450/462], loss: 0.0065\n",
      "    step: [50/237], loss: 0.0982\n",
      "    step: [100/237], loss: 0.1434\n",
      "    step: [150/237], loss: 0.0654\n",
      "    step: [200/237], loss: 0.1976\n",
      "    step: [50/198], loss: 0.1685\n",
      "    step: [100/198], loss: 0.0319\n",
      "    step: [150/198], loss: 0.0779\n",
      "Training done\n"
     ]
    }
   ],
   "source": [
    "# Same learning rate\n",
    "optimizer = torch.optim.Adam(lstm_nn.parameters(), lr=LEARNING_RATE)\n",
    "for epoch in np.arange(NUM_EPOCHS):\n",
    "    print('Epoch [%d/%d]' %(epoch+1, NUM_EPOCHS))    \n",
    "    for f in training_files:\n",
    "        train_loader = DataLoader(dataset=training_sets[f], batch_size=BATCH_SIZE, shuffle=False)    \n",
    "        lstm_nn.init_hidden()\n",
    "\n",
    "        for i, (X, y) in enumerate(train_loader):\n",
    "            if (len(X) != BATCH_SIZE):\n",
    "                continue\n",
    "\n",
    "            data = Variable(X.view(-1, 1, INPUT_SIZE))\n",
    "            target = Variable(y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction = lstm_nn(data)\n",
    "            loss = criterion(prediction, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % BATCH_SIZE == 0:                \n",
    "                print('    step: [%d/%d], loss: %.4f'\n",
    "                      %(i+1, len(training_sets[f].target_tensor)//BATCH_SIZE, loss.data[0]))\n",
    "\n",
    "print('Training done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model parameters\n",
    "\n",
    "Parameters will be used by the driver to get a command to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_nn.state_dict(), 'rnn_params.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type RNN_LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(lstm_nn, 'whole_net.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in training_files:\n",
    "#     # Train the network for each training session\n",
    "#     print('Training file: %s' %(file))\n",
    "    \n",
    "#     train_ds = pd.read_csv(file)\n",
    "#     X_train = train_ds.iloc[:, 3:].values\n",
    "#     y_train = train_ds.iloc[:, :3].values\n",
    "    \n",
    "#     imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "#     imputer = imputer.fit(X_train)\n",
    "#     X_train = imputer.transform(X_train)\n",
    "    \n",
    "#     X_train = torch.from_numpy(X_train).float()\n",
    "#     y_train = torch.from_numpy(y_train).float()\n",
    "    \n",
    "#     dataset = TensorDataset(X_train, y_train)\n",
    "    \n",
    "#     train_loader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "#     rnn.init_hidden()\n",
    "    \n",
    "#     for epoch in range(NUM_EPOCHS):\n",
    "#         for i, (X, y) in enumerate(train_loader):\n",
    "#             if (len(X) != BATCH_SIZE):\n",
    "#                 continue\n",
    "            \n",
    "#             data = Variable(X.view(-1, 1, INPUT_SIZE))\n",
    "#             target = Variable(y)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             prediction = rnn(data)\n",
    "#             loss = criterion(prediction, target)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             if (i+1) % 30 == 0:\n",
    "#                 print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "#                       %(epoch+1, NUM_EPOCHS, i+1, len(X_train)//BATCH_SIZE, loss.data[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
